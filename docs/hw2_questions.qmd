---
title: "Poisson Regression Examples"
author: "Sarah Howard"
date: 12 May 2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---
## Blueprinty Case Study

### Data


### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

```python
#| label: load-blueprinty-data
#| echo: true
import pandas as pd

# Load the dataset
blueprinty_df = pd.read_csv("blueprinty.csv")
blueprinty_df['region'] = blueprinty_df['region'].astype('category')

```


```python
#| label: histogram-patents
#| fig-cap: "Histogram of Patents by Customer Status"
#| echo: true
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style="whitegrid")
plt.figure(figsize=(8, 5))
sns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='dodge', bins=15)
plt.title('Distribution of Patents by Customer Status')
plt.xlabel('Number of Patents')
plt.ylabel('Count')
plt.show()

```


    
![png](output_1_0.png)
    



```python
#| label: mean-patents
#| echo: true
# Compare mean number of patents by customer status
blueprinty_df.groupby('iscustomer')['patents'].mean()

```




    iscustomer
    0    3.473013
    1    4.133056
    Name: patents, dtype: float64




```python
#| label: boxplot-age
#| fig-cap: "Boxplot of Firm Age by Customer Status"
#| echo: true
plt.figure(figsize=(6, 4))
sns.boxplot(data=blueprinty_df, x='iscustomer', y='age')
plt.title('Firm Age by Customer Status')
plt.xlabel('Customer Status (0 = Non, 1 = Yes)')
plt.ylabel('Firm Age')
plt.show()

```


    
![png](output_3_0.png)
    



```python
#| label: age-summary
#| echo: true
# Summary statistics of firm age by customer status
blueprinty_df.groupby('iscustomer')['age'].describe()

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>count</th>
      <th>mean</th>
      <th>std</th>
      <th>min</th>
      <th>25%</th>
      <th>50%</th>
      <th>75%</th>
      <th>max</th>
    </tr>
    <tr>
      <th>iscustomer</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1019.0</td>
      <td>26.101570</td>
      <td>6.945426</td>
      <td>9.0</td>
      <td>21.0</td>
      <td>25.5</td>
      <td>31.25</td>
      <td>47.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>481.0</td>
      <td>26.900208</td>
      <td>7.814678</td>
      <td>10.0</td>
      <td>20.5</td>
      <td>26.5</td>
      <td>32.50</td>
      <td>49.0</td>
    </tr>
  </tbody>
</table>
</div>




```python
#| label: region-distribution
#| fig-cap: "Regional Distribution of Firms by Customer Status"
#| echo: true
plt.figure(figsize=(8, 5))
sns.countplot(data=blueprinty_df, x='region', hue='iscustomer')
plt.title('Region by Customer Status')
plt.xlabel('Region')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

```


    
![png](output_5_0.png)
    



```python
#| label: region-summary
#| echo: true
# Tabulate number of firms per region by customer status
blueprinty_df.groupby(['region', 'iscustomer']).size().unstack()

```

    /tmp/ipykernel_48627/3320520734.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.
      blueprinty_df.groupby(['region', 'iscustomer']).size().unstack()





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>iscustomer</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>region</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Midwest</th>
      <td>187</td>
      <td>37</td>
    </tr>
    <tr>
      <th>Northeast</th>
      <td>273</td>
      <td>328</td>
    </tr>
    <tr>
      <th>Northwest</th>
      <td>158</td>
      <td>29</td>
    </tr>
    <tr>
      <th>South</th>
      <td>156</td>
      <td>35</td>
    </tr>
    <tr>
      <th>Southwest</th>
      <td>245</td>
      <td>52</td>
    </tr>
  </tbody>
</table>
</div>



### Observations and Interpretation

Firms that use Blueprinty's software appear to differ systematically from those that do not in several key ways:

1. **Patent Output**  
   - Blueprinty customers have a higher average number of patents (~4.13) compared to non-customers (~3.47).  
   - The distribution is skewed right for both groups, but customers are slightly more likely to have higher patent counts.  
   - This could suggest that Blueprinty customers are more productive or successful—but we must be cautious before attributing this difference to software usage alone.

2. **Firm Age**  
   - Customers are slightly older on average (mean age ~26.9 years) compared to non-customers (~26.1 years).  
   - The difference is not dramatic, but could reflect that more established firms are more likely to adopt Blueprinty’s tools.

3. **Regional Location**  
   - Blueprinty’s customer base is heavily concentrated in the **Northeast** region, which accounts for 68% of customer firms.  
   - In contrast, the Northeast only represents 27% of non-customer firms.  
   - This concentration suggests regional adoption trends and also indicates that region must be controlled for in the


Since our outcome variable of interest—the number of patents awarded—is a non-negative integer count over a fixed time period (5 years), it is well-suited to modeling using the Poisson distribution.

The probability mass function of the Poisson distribution is:

$$
f(Y \mid \lambda) = \frac{e^{-\lambda} \lambda^Y}{Y!}
$$

The corresponding log-likelihood for a sample of \( n \) independent observations is:

$$
\log L(\lambda) = \sum_{i=1}^n \left( Y_i \log \lambda - \lambda - \log(Y_i!) \right)
$$

Below, we define this log-likelihood in Python and visualize how it changes across values of \( \lambda \).


### Estimation of Poisson Regression Model

```python
#| label: poisson-loglikelihood-plot
#| fig-cap: "Poisson Log-Likelihood as a Function of Lambda"
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import factorial

Y_obs = blueprinty_df["patents"].values

def poisson_loglikelihood(lmbda, Y):
    if lmbda <= 0:
        return -np.inf
    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))

lambda_vals = np.linspace(0.1, 10, 200)
loglik_vals = [poisson_loglikelihood(lmbda, Y_obs) for lmbda in lambda_vals]

plt.figure(figsize=(8, 5))
plt.plot(lambda_vals, loglik_vals)
plt.xlabel("Lambda (λ)")
plt.ylabel("Log-Likelihood")
plt.title("Poisson Log-Likelihood vs. Lambda")
plt.grid(True)
plt.show()

```


    
![png](output_9_0.png)
    


We can analytically derive the MLE for \( \lambda \) by setting the derivative of the log-likelihood equal to zero. This yields:

$$
\hat{\lambda}_{MLE} = \bar{Y}
$$

This result makes intuitive sense, as the Poisson distribution's mean is \( \lambda \). Let's compute this both analytically and via numerical optimization:



```python
#| label: poisson-mle-estimation
#| echo: true
from scipy.optimize import minimize

lambda_mle_analytic = np.mean(Y_obs)

def neg_loglik(lmbda):
    return -poisson_loglikelihood(lmbda[0], Y_obs)

result = minimize(neg_loglik, x0=[1.0], bounds=[(1e-6, None)])
lambda_mle_numerical = result.x[0]

lambda_mle_analytic, lambda_mle_numerical

```




    (3.6846666666666668, 3.684666485763343)



We now extend the Poisson model to include covariates using a log-linear model:

$$
Y_i \sim \text{Poisson}(\lambda_i), \quad \text{where } \lambda_i = \exp(X_i^\top \beta)
$$

This allows the expected number of patents to depend on firm characteristics such as age, region, and Blueprinty customer status. We estimate the model parameters using Maximum Likelihood Estimation (MLE).


We fit a Poisson regression model to estimate the effect of firm characteristics on the number of awarded patents. The model includes:

- Age and age squared (to capture nonlinear age effects),
- Region fixed effects (with Midwest as the reference category),
- A binary indicator for whether the firm uses Blueprinty software.

The coefficient on `iscustomer` is approximately **0.208**, and is statistically significant. This suggests that, all else equal, being a Blueprinty customer is associated with a higher expected number of patents.


## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped data from 40,000 Airbnb listings in New York City. The dataset includes variables such as room type, price, number of reviews, and review scores.

We assume that the number of reviews is a good proxy for the number of bookings. In this analysis, we conduct exploratory data analysis (EDA), clean the dataset, fit a Poisson regression model, and interpret the results.

### Data Cleaning

We removed rows with missing values in key variables: number of reviews, room type, bathrooms, bedrooms, days listed, price, review scores (cleanliness, location, value), and instant bookability. The cleaned dataset includes 30,160 listings.

### Exploratory Data Analysis

- **Number of Reviews**: Highly right-skewed, ranging from 1 to 421.
- **Room Type**: Entire homes/apt receive the most reviews on average, followed by private rooms and then shared rooms.
- **Price**: Weak negative relationship with review count. Cheaper listings tend to receive more reviews.

#### Visualizations

![Distribution of Number of Reviews](distribution_reviews.png)

![Number of Reviews by Room Type](reviews_by_room_type.png)

![Number of Reviews vs. Price](reviews_vs_price.png)

### Poisson Regression Model

We model the number of reviews using a Poisson regression:

```r
number_of_reviews ~ C(room_type) + bathrooms + bedrooms + days + price +
                    review_scores_cleanliness + review_scores_location +
                    review_scores_value + instant_bookable
```

#### Key Coefficients:

- **Private room**: 1% fewer reviews than entire homes (small effect)
- **Shared room**: ~22% fewer reviews
- **Bathrooms**: More bathrooms are associated with ~11% fewer reviews
- **Bedrooms**: Each additional bedroom increases reviews by ~7%
- **Days listed**: Longer listing time strongly increases reviews
- **Price**: Slight negative effect
- **Cleanliness Score**: Strong positive effect (~11% increase per unit)
- **Location & Value Scores**: Unexpected negative effects (may suggest multicollinearity)
- **Instant Bookable**: ~41% more reviews (`exp(0.3459) \approx 1.41`)

### Interpretation

- Listings that are older, cheaper, and allow instant booking receive more reviews.
- Cleanliness plays a major role in guest satisfaction and bookings.
- More bedrooms generally mean more reviews, while more bathrooms might be a confounding factor.
- Review score effects may benefit from further investigation with interaction terms or alternative model specifications.