[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sarah Howard",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Howard Website HW1",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription"
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You‚Äôll then calculate a vector of 10,000 differences, and then you‚Äôll plot the cumulative average of that vector of differences. This average will likely be ‚Äúnoisey‚Äù when only averaging a few numbers, but should ‚Äúsettle down‚Äù and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g.¬†50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Sarah Howard",
    "section": "",
    "text": "title: ‚ÄúResume‚Äù format: hmtl: default typst: default Download PDF file. Welcome to my website!"
  },
  {
    "objectID": "hw1_questions.html#overview",
    "href": "hw1_questions.html#overview",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Overview",
    "text": "Overview\nThis dataset contains data from a large-scale natural field experiment investigating how different types of matching grants influence charitable giving.\n\nTotal Observations: 50,083 individuals\n\nVariables: 51\n\nSource: Does Price Matter in Charitable Giving? (Karlan & List, 2007)"
  },
  {
    "objectID": "hw1_questions.html#experimental-design-variables",
    "href": "hw1_questions.html#experimental-design-variables",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üéØ Experimental Design Variables",
    "text": "üéØ Experimental Design Variables\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\n1 if participant received a matching grant offer, 0 if control group\n\n\nratio\nCategorical: Match ratio (e.g., 1:1, 2:1, 3:1)\n\n\nratio2, ratio3\nDummies for 2:1 and 3:1 match conditions\n\n\nsize\nCategorical: Maximum match size ($25k, $50k, $100k, or unstated)\n\n\nsize25, size50, size100, sizeno\nDummy variables for match size\n\n\nask1, ask2, ask3\nSuggested donation amounts (based on prior giving)"
  },
  {
    "objectID": "hw1_questions.html#donation-behavior-variables",
    "href": "hw1_questions.html#donation-behavior-variables",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üí∏ Donation Behavior Variables",
    "text": "üí∏ Donation Behavior Variables\n\n\n\nVariable\nDescription\n\n\n\n\ngave\nBinary: 1 if donated, 0 otherwise\n\n\namount\nAmount donated (USD)\n\n\namountchange\nChange in amount donated vs.¬†previous gift"
  },
  {
    "objectID": "hw1_questions.html#demographic-zip-code-level-data",
    "href": "hw1_questions.html#demographic-zip-code-level-data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üßë‚Äçü§ù‚Äçüßë Demographic & ZIP Code-level Data",
    "text": "üßë‚Äçü§ù‚Äçüßë Demographic & ZIP Code-level Data\n\n\n\nVariable\nDescription\n\n\n\n\npwhite, pblack\nProportion of white and Black residents\n\n\npage18_39\nProportion aged 18‚Äì39\n\n\nave_hh_sz\nAverage household size\n\n\nmedian_hhincome\nMedian household income\n\n\npowner\nProportion of homeowners\n\n\npsch_atlstba\nProportion with at least a bachelor‚Äôs degree\n\n\npop_propurban\nProportion of population in urban areas"
  },
  {
    "objectID": "hw1_questions.html#political-context-variables",
    "href": "hw1_questions.html#political-context-variables",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üó≥Ô∏è Political Context Variables",
    "text": "üó≥Ô∏è Political Context Variables\n\n\n\nVariable\nDescription\n\n\n\n\nred0, blue0\nBinary: Red or blue state indicator\n\n\nredcty, bluecty\nBinary: Red or blue county indicator"
  },
  {
    "objectID": "hw1_questions.html#missing-data",
    "href": "hw1_questions.html#missing-data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "‚ùó Missing Data",
    "text": "‚ùó Missing Data\nSome variables (especially demographic ones) have missing values for ~2,000 cases due to ZIP-level data availability.\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\nBalance Test\n\n\n‚úÖ Randomization Check: Treatment vs.¬†Control Group Balance\nTo test the integrity of the random assignment, we compared several background variables between treatment and control groups using t-tests.\n\nüîç Variables Tested:\n\nmrm2: Months since last donation\n\nyears: Years since first donation\n\nhpa: Highest prior donation\n\nfemale: Female indicator\n\ncouple: Couple indicator\n\n\n\nüìà Key Findings:\n\nNo statistically significant differences (p &gt; 0.05) were found between groups for any variable tested.\nFor mrm2, both a t-test and a linear regression (mrm2 ~ treatment) produced the same result (p = 0.905), confirming the methods agree.\n\n\n\nüß† Why It Matters:\nThese tests confirm that the treatment and control groups were statistically similar before the intervention ‚Äî supporting the internal validity of the experiment.\nThis is the purpose of Table 1 in the paper: to demonstrate that any differences in donation behavior can be attributed to the matching grant treatment, not pre-existing group differences.\n\n\n\nExperimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\nüìä Response Rate by Treatment Group\nWe visualize whether being offered a matching donation affects the likelihood of donating. This barplot compares the donation response rate between the treatment and control groups.\n\nimport pandas as pd\n\n# Load the dataset (make sure the file is in the same directory as your notebook)\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Take a quick look at the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\n\n\ntreatment\n\n\ncontrol\n\n\nratio\n\n\nratio2\n\n\nratio3\n\n\nsize\n\n\nsize25\n\n\nsize50\n\n\nsize100\n\n\nsizeno\n\n\n‚Ä¶\n\n\nredcty\n\n\nbluecty\n\n\npwhite\n\n\npblack\n\n\npage18_39\n\n\nave_hh_sz\n\n\nmedian_hhincome\n\n\npowner\n\n\npsch_atlstba\n\n\npop_propurban\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.446493\n\n\n0.527769\n\n\n0.317591\n\n\n2.10\n\n\n28517.0\n\n\n0.499807\n\n\n0.324528\n\n\n1.0\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$100,000\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.935706\n\n\n0.011948\n\n\n0.276128\n\n\n2.48\n\n\n51175.0\n\n\n0.721941\n\n\n0.192668\n\n\n1.0\n\n\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\nUnstated\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\n0.888331\n\n\n0.010760\n\n\n0.279412\n\n\n2.65\n\n\n79269.0\n\n\n0.920431\n\n\n0.412142\n\n\n1.0\n\n\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$50,000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.759014\n\n\n0.127421\n\n\n0.442389\n\n\n1.85\n\n\n40908.0\n\n\n0.416072\n\n\n0.439965\n\n\n1.0\n\n\n\n\n\n5 rows √ó 51 columns\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the mean donation rate for each group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"Group\"] = donation_rates[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n# Label the chart\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.title(\"Donation Response Rate: Treatment vs Control\")\nplt.ylim(0, 0.03)  # Set y-axis range for visual clarity\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n/tmp/ipykernel_88776/1362242253.py:10: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1451/3587301648.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n\n\npng\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Prepare ratio indicators\ndf[\"ratio\"] = df[\"ratio\"].astype(\"str\")\ndf[\"ratio1\"] = (df[\"ratio\"] == \"1\").astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == \"2\").astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == \"3\").astype(int)\n\n# Filter for treatment group only (exclude control group)\ndf_treat_only = df[df[\"treatment\"] == 1].copy()\n\n# T-tests: 1:1 vs 2:1 and 2:1 vs 3:1\ngave_1to1 = df_treat_only[df_treat_only[\"ratio1\"] == 1][\"gave\"]\ngave_2to1 = df_treat_only[df_treat_only[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df_treat_only[df_treat_only[\"ratio3\"] == 1][\"gave\"]\n\nt_1v2, p_1v2, _ = sm.stats.ttest_ind(gave_1to1, gave_2to1)\nt_2v3, p_2v3, _ = sm.stats.ttest_ind(gave_2to1, gave_3to1)\n\n# Regression using dummy variables\nreg_dummy = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=df_treat_only).fit()\n\n# Regression using categorical variable\ndf_treat_only[\"ratio\"] = df_treat_only[\"ratio\"].astype(\"category\")\nreg_cat = smf.ols(\"gave ~ C(ratio)\", data=df_treat_only).fit()\n\n# Group means\ngroup_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\ndirect_diff_1v2 = group_means[\"2\"] - group_means[\"1\"]\ndirect_diff_2v3 = group_means[\"3\"] - group_means[\"2\"]\n\n# Regression differences\ncoef_1 = reg_dummy.params[\"ratio1\"]\ncoef_2 = reg_dummy.params[\"ratio2\"]\ncoef_3 = reg_dummy.params[\"ratio3\"]\nreg_diff_1v2 = coef_2 - coef_1\nreg_diff_2v3 = coef_3 - coef_2\n\n# Print summary\nprint(\"T-test p-value (1:1 vs 2:1):\", p_1v2)\nprint(\"T-test p-value (2:1 vs 3:1):\", p_2v3)\nprint(\"Direct difference in response rates (2:1 - 1:1):\", direct_diff_1v2)\nprint(\"Direct difference in response rates (3:1 - 2:1):\", direct_diff_2v3)\nprint(\"Regression-based difference (2:1 - 1:1):\", reg_diff_1v2)\nprint(\"Regression-based difference (3:1 - 2:1):\", reg_diff_2v3)\n\n# Optional: Show regression summaries\nprint(\"\\nOLS Regression with dummy variables:\")\nprint(reg_dummy.summary())\n\nprint(\"\\nOLS Regression with categorical variable:\")\nprint(reg_cat.summary())\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 14 May 2025   Prob (F-statistic):              0.524\nTime:                        12:18:14   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 14 May 2025   Prob (F-statistic):              0.524\nTime:                        12:18:14   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_88776/2038281865.py:33: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_1451/3381687197.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\n\n\nüß™ Does Match Size Affect Donation Rates?\nWe test whether offering larger match ratios (e.g., 2:1 or 3:1) increases the likelihood of donating compared to a standard 1:1 match.\n\nüîç T-Test Results\n\n1:1 vs 2:1: No statistically significant difference (p ‚âà 0.335)\n2:1 vs 3:1: No statistically significant difference (p ‚âà 0.960)\n\n\n\nüìà Regression Results\nWe ran two regressions: 1. Using separate dummy variables (ratio1, ratio2, ratio3) ‚Äî one for each match level 2. Using a single categorical variable (C(ratio))\nBoth approaches yielded similar results: - Donation rate for 1:1 ‚âà 2.07% - Donation rate for 2:1 ‚âà 2.26% - Donation rate for 3:1 ‚âà 2.27% - Differences between them are very small and not statistically significant\n\n\nüìä Direct Comparison of Response Rates\n\n2:1 ‚Äì 1:1 ‚âà +0.19 percentage points\n3:1 ‚Äì 2:1 ‚âà +0.01 percentage points These findings match the regression results.\n\n\n\n‚úÖ Conclusion\nThese results support the authors‚Äô observatin that ‚ÄúThe figures suggest that larger match ratios have no additional impact.‚Äù\nüí° Key Insight: Donors respond to the presence of a match, but increasing the size of the match does not further increase the likelihood of donating.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter only those who made a donation\ndf_donors = df[df[\"gave\"] == 1]\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Histogram for Control Group\nplt.subplot(1, 2, 1)\ncontrol_amounts = df_donors[df_donors[\"treatment\"] == 0][\"amount\"]\nsns.histplot(control_amounts, bins=30, kde=False, color=\"skyblue\")\nplt.axvline(control_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${control_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Control Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Histogram for Treatment Group\nplt.subplot(1, 2, 2)\ntreatment_amounts = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\nsns.histplot(treatment_amounts, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(treatment_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${treatment_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Final layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Load the dataset (make sure the file is in the same directory as your notebook)\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Take a quick look at the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\n\n\ncontrol\n\n\nratio\n\n\nratio2\n\n\nratio3\n\n\nsize\n\n\nsize25\n\n\nsize50\n\n\nsize100\n\n\nsizeno\n\n\n‚Ä¶\n\n\nredcty\n\n\nbluecty\n\n\npwhite\n\n\npblack\n\n\npage18_39\n\n\nave_hh_sz\n\n\nmedian_hhincome\n\n\npowner\n\n\npsch_atlstba\n\n\npop_propurban\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.446493\n\n\n0.527769\n\n\n0.317591\n\n\n2.10\n\n\n28517.0\n\n\n0.499807\n\n\n0.324528\n\n\n1.0\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$100,000\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.935706\n\n\n0.011948\n\n\n0.276128\n\n\n2.48\n\n\n51175.0\n\n\n0.721941\n\n\n0.192668\n\n\n1.0\n\n\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\nUnstated\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\n0.888331\n\n\n0.010760\n\n\n0.279412\n\n\n2.65\n\n\n79269.0\n\n\n0.920431\n\n\n0.412142\n\n\n1.0\n\n\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$50,000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.759014\n\n\n0.127421\n\n\n0.442389\n\n\n1.85\n\n\n40908.0\n\n\n0.416072\n\n\n0.439965\n\n\n1.0\n\n\n\n\n\n5 rows √ó 51 columns\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the mean donation rate for each group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"Group\"] = donation_rates[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n# Label the chart\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.title(\"Donation Response Rate: Treatment vs Control\")\nplt.ylim(0, 0.03)  # Set y-axis range for visual clarity\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n/tmp/ipykernel_1451/3587301648.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n\n\npng\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Prepare ratio indicators\ndf[\"ratio\"] = df[\"ratio\"].astype(\"str\")\ndf[\"ratio1\"] = (df[\"ratio\"] == \"1\").astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == \"2\").astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == \"3\").astype(int)\n\n# Filter for treatment group only (exclude control group)\ndf_treat_only = df[df[\"treatment\"] == 1].copy()\n\n# T-tests: 1:1 vs 2:1 and 2:1 vs 3:1\ngave_1to1 = df_treat_only[df_treat_only[\"ratio1\"] == 1][\"gave\"]\ngave_2to1 = df_treat_only[df_treat_only[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df_treat_only[df_treat_only[\"ratio3\"] == 1][\"gave\"]\n\nt_1v2, p_1v2, _ = sm.stats.ttest_ind(gave_1to1, gave_2to1)\nt_2v3, p_2v3, _ = sm.stats.ttest_ind(gave_2to1, gave_3to1)\n\n# Regression using dummy variables\nreg_dummy = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=df_treat_only).fit()\n\n# Regression using categorical variable\ndf_treat_only[\"ratio\"] = df_treat_only[\"ratio\"].astype(\"category\")\nreg_cat = smf.ols(\"gave ~ C(ratio)\", data=df_treat_only).fit()\n\n# Group means\ngroup_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\ndirect_diff_1v2 = group_means[\"2\"] - group_means[\"1\"]\ndirect_diff_2v3 = group_means[\"3\"] - group_means[\"2\"]\n\n# Regression differences\ncoef_1 = reg_dummy.params[\"ratio1\"]\ncoef_2 = reg_dummy.params[\"ratio2\"]\ncoef_3 = reg_dummy.params[\"ratio3\"]\nreg_diff_1v2 = coef_2 - coef_1\nreg_diff_2v3 = coef_3 - coef_2\n\n# Print summary\nprint(\"T-test p-value (1:1 vs 2:1):\", p_1v2)\nprint(\"T-test p-value (2:1 vs 3:1):\", p_2v3)\nprint(\"Direct difference in response rates (2:1 - 1:1):\", direct_diff_1v2)\nprint(\"Direct difference in response rates (3:1 - 2:1):\", direct_diff_2v3)\nprint(\"Regression-based difference (2:1 - 1:1):\", reg_diff_1v2)\nprint(\"Regression-based difference (3:1 - 2:1):\", reg_diff_2v3)\n\n# Optional: Show regression summaries\nprint(\"\\nOLS Regression with dummy variables:\")\nprint(reg_dummy.summary())\n\nprint(\"\\nOLS Regression with categorical variable:\")\nprint(reg_cat.summary())\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_1451/3381687197.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\n\n\n\nüß™ Does Match Size Affect Donation Rates?\nWe test whether offering larger match ratios (e.g., 2:1 or 3:1) increases the likelihood of donating compared to a standard 1:1 match.\n\nüîç T-Test Results\n\n1:1 vs 2:1: No statistically significant difference (p ‚âà 0.335)\n2:1 vs 3:1: No statistically significant difference (p ‚âà 0.960)\n\n\n\nüìà Regression Results\nWe ran two regressions: 1. Using separate dummy variables (ratio1, ratio2, ratio3) ‚Äî one for each match level 2. Using a single categorical variable (C(ratio))\nBoth approaches yielded similar results: - Donation rate for 1:1 ‚âà 2.07% - Donation rate for 2:1 ‚âà 2.26% - Donation rate for 3:1 ‚âà 2.27% - Differences between them are very small and not statistically significant\n\n\nüìä Direct Comparison of Response Rates\n\n2:1 ‚Äì 1:1 ‚âà +0.19 percentage points\n3:1 ‚Äì 2:1 ‚âà +0.01 percentage points These findings match the regression results.\n\n\n\n‚úÖ Conclusion\nThese results support the authors‚Äô observatin that ‚ÄúThe figures suggest that larger match ratios have no additional impact.‚Äù\nüí° Key Insight: Donors respond to the presence of a match, but increasing the size of the match does not further increase the likelihood of donating.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter only those who made a donation\ndf_donors = df[df[\"gave\"] == 1]\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Histogram for Control Group\nplt.subplot(1, 2, 1)\ncontrol_amounts = df_donors[df_donors[\"treatment\"] == 0][\"amount\"]\nsns.histplot(control_amounts, bins=30, kde=False, color=\"skyblue\")\nplt.axvline(control_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${control_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Control Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Histogram for Treatment Group\nplt.subplot(1, 2, 2)\ntreatment_amounts = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\nsns.histplot(treatment_amounts, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(treatment_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${treatment_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Final layout\nplt.tight_layout()\nplt.show()\n\n\n\npng\n\n\n\n\n\nüíµ Size of Charitable Contribution\n\n‚úÖ Q1: Does treatment affect donation amount (all individuals)?\nWe performed both a t-test and a bivariate linear regression on the full dataset.\n\nT-test p-value: ~0.063\nRegression coefficient: +0.15 (Treatment vs.¬†Control)\nüìâ Interpretation:\n\nThe treatment group gave slightly more on average, but the difference is not statistically significant at the 5% level.\nThis suggests that while the match offer encourages more people to donate, it does not meaningfully affect how much they give, on average, across the full sample.\n\n\n\n\n\n‚úÖ Q2: Does treatment affect donation amount among donors only?\nWe repeated the analysis only for individuals who made a donation (i.e., gave == 1).\n\nT-test p-value: ~0.561\nRegression coefficient: ‚Äì1.67\nüìâ Interpretation:\n\nAmong donors, there is no statistically significant difference in how much was donated between the treatment and control groups.\nInterestingly, the control group gave slightly more, but this difference is small and not reliable.\n‚ö†Ô∏è Causal Note: This analysis does not have a causal interpretation, because it conditions on making a donation ‚Äî a behavior affected by the treatment. This introduces selection bias.\n\n\n\n\n\n‚úÖ Q3: What do the histograms show?\nWe created histograms of donation amounts among donors, separately for the treatment and control groups. Each plot includes:\n\nA red dashed line indicating the mean donation.\nThe distributions are highly right-skewed, with many small gifts and a few large ones.\n\nüìä Observations: - The average donation amount is very similar across the groups. - Most donations are clustered around $10‚Äì$50. - There is no visual evidence that treatment led to larger donations.\n\n\n\nüß† Final Takeaway:\nOffering a matching grant increases the probability of giving, but among those who give, it does not increase the amount given. This suggests that match offers primarily work as a participation nudge, not a generosity multiplier.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 Bernoulli trials\ncontrol_draws = np.random.binomial(1, 0.018, size=10000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\n# Calculate the difference at each draw\ndifferences = treatment_draws - control_draws\n\n# Compute the cumulative average of the differences\ncumulative_average = np.cumsum(differences) / np.arange(1, 10001)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_average, label=\"Cumulative Average of Differences\")\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.title(\"Law of Large Numbers: Cumulative Average of Treatment - Control\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\npng\n\n\n\n\n\nüìà Law of Large Numbers: Simulating Donation Response Rates\nIn this simulation, we illustrate the Law of Large Numbers using synthetic data inspired by the charitable giving experiment.\n\nüéØ Setup:\n\nControl Group: Simulated with a Bernoulli distribution where the probability of donating is 1.8% (p = 0.018)\nTreatment Group: Simulated with a Bernoulli distribution where the probability of donating is 2.2% (p = 0.022)\nWe simulate 10,000 draws from each group and compute the difference (Treatment ‚Äì Control) for each pair.\nThen, we plot the cumulative average of those differences over time.\n\n\n\nüìä What the Graph Shows:\n\nThe line begins noisy due to early randomness.\nAs the number of draws increases, the average stabilizes and converges around the true treatment effect of 0.004 (2.2% - 1.8%).\nThe red dashed line marks this theoretical value.\n\n\n\nüß† Interpretation:\nThis visualization demonstrates the Law of Large Numbers: &gt; As the sample size grows, the sample average of a statistic will converge to its true population value.\nThis underlines why large-scale experiments (like the one in the Karlan & List paper) are powerful: with enough data, we can estimate effects reliably despite inherent randomness in individual behavior.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Function to simulate the distribution of mean differences\ndef simulate_diff_distribution(sample_size, reps=1000):\n    differences = []\n    for _ in range(reps):\n        control_sample = np.random.binomial(1, 0.018, size=sample_size)\n        treatment_sample = np.random.binomial(1, 0.022, size=sample_size)\n        diff = treatment_sample.mean() - control_sample.mean()\n        differences.append(diff)\n    return differences\n\n# Sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Generate and plot each histogram separately\nfor size in sample_sizes:\n    diffs = simulate_diff_distribution(sample_size=size)\n    plt.figure(figsize=(7, 4))\n    sns.histplot(diffs, bins=30, kde=False, color=\"skyblue\")\n    plt.axvline(0.004, color='red', linestyle='--', linewidth=2, label=\"True Effect = 0.004\")\n    plt.title(f\"Sampling Distribution of Mean Differences (n = {size})\")\n    plt.xlabel(\"Mean Difference (Treatment - Control)\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\nüìä Central Limit Theorem Demonstration\nThis section visually demonstrates the Central Limit Theorem (CLT) using simulations based on the charitable giving experiment setup.\n\nüî¨ Method:\nFor each sample size ‚Äî n = 50, 200, 500, 1000: 1. We simulate 1,000 experiments. 2. In each experiment: - Take n samples from the control group (Bernoulli, p = 0.018) - Take n samples from the treatment group (Bernoulli, p = 0.022) - Compute the average difference in donation rates: treatment_mean - control_mean 3. We plot the histogram of the 1,000 average differences.\n\n\nüìà Interpretation of Histograms:\n\nFor n = 50, the distribution is wide and irregular ‚Äî highly affected by sampling noise.\nAs n increases (200, 500, 1000):\n\nThe distribution becomes tighter and smoother\nIt becomes centered around the true effect of 0.004 (shown by a red dashed line).\nThe shape begins to resemble a normal distribution.\n\n\n\n\nüß† Why It Matters:\nThis simulation illustrates the Central Limit Theorem: &gt; As sample size increases, the sampling distribution of the sample mean becomes approximately normal ‚Äî even when the underlying data are not normally distributed.\n‚úÖ Takeaway:\nThanks to the CLT, we can use normal-based inference methods (like t-tests and regression) when we have large enough samples ‚Äî as in the Karlan & List field experiment."
  },
  {
    "objectID": "hw1_questions.html#randomization-check-treatment-vs.-control-group-balance",
    "href": "hw1_questions.html#randomization-check-treatment-vs.-control-group-balance",
    "title": "A Replication of Karlan and List (2007)",
    "section": "‚úÖ Randomization Check: Treatment vs.¬†Control Group Balance",
    "text": "‚úÖ Randomization Check: Treatment vs.¬†Control Group Balance\nTo test the integrity of the random assignment, we compared several background variables between treatment and control groups using t-tests.\n\nüîç Variables Tested:\n\nmrm2: Months since last donation\n\nyears: Years since first donation\n\nhpa: Highest prior donation\n\nfemale: Female indicator\n\ncouple: Couple indicator\n\n\n\nüìà Key Findings:\n\nNo statistically significant differences (p &gt; 0.05) were found between groups for any variable tested.\nFor mrm2, both a t-test and a linear regression (mrm2 ~ treatment) produced the same result (p = 0.905), confirming the methods agree.\n\n\n\nüß† Why It Matters:\nThese tests confirm that the treatment and control groups were statistically similar before the intervention ‚Äî supporting the internal validity of the experiment.\nThis is the purpose of Table 1 in the paper: to demonstrate that any differences in donation behavior can be attributed to the matching grant treatment, not pre-existing group differences."
  },
  {
    "objectID": "hw1_questions.html#response-rate-by-treatment-group",
    "href": "hw1_questions.html#response-rate-by-treatment-group",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üìä Response Rate by Treatment Group",
    "text": "üìä Response Rate by Treatment Group\nWe visualize whether being offered a matching donation affects the likelihood of donating. This barplot compares the donation response rate between the treatment and control groups.\n\nimport pandas as pd\n\n# Load the dataset (make sure the file is in the same directory as your notebook)\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Take a quick look at the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\n\n\ntreatment\n\n\ncontrol\n\n\nratio\n\n\nratio2\n\n\nratio3\n\n\nsize\n\n\nsize25\n\n\nsize50\n\n\nsize100\n\n\nsizeno\n\n\n‚Ä¶\n\n\nredcty\n\n\nbluecty\n\n\npwhite\n\n\npblack\n\n\npage18_39\n\n\nave_hh_sz\n\n\nmedian_hhincome\n\n\npowner\n\n\npsch_atlstba\n\n\npop_propurban\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.446493\n\n\n0.527769\n\n\n0.317591\n\n\n2.10\n\n\n28517.0\n\n\n0.499807\n\n\n0.324528\n\n\n1.0\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$100,000\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.935706\n\n\n0.011948\n\n\n0.276128\n\n\n2.48\n\n\n51175.0\n\n\n0.721941\n\n\n0.192668\n\n\n1.0\n\n\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\nUnstated\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\n0.888331\n\n\n0.010760\n\n\n0.279412\n\n\n2.65\n\n\n79269.0\n\n\n0.920431\n\n\n0.412142\n\n\n1.0\n\n\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$50,000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.759014\n\n\n0.127421\n\n\n0.442389\n\n\n1.85\n\n\n40908.0\n\n\n0.416072\n\n\n0.439965\n\n\n1.0\n\n\n\n\n\n5 rows √ó 51 columns\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the mean donation rate for each group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"Group\"] = donation_rates[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n# Label the chart\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.title(\"Donation Response Rate: Treatment vs Control\")\nplt.ylim(0, 0.03)  # Set y-axis range for visual clarity\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n/tmp/ipykernel_88776/1362242253.py:10: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1451/3587301648.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n\n\npng\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Prepare ratio indicators\ndf[\"ratio\"] = df[\"ratio\"].astype(\"str\")\ndf[\"ratio1\"] = (df[\"ratio\"] == \"1\").astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == \"2\").astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == \"3\").astype(int)\n\n# Filter for treatment group only (exclude control group)\ndf_treat_only = df[df[\"treatment\"] == 1].copy()\n\n# T-tests: 1:1 vs 2:1 and 2:1 vs 3:1\ngave_1to1 = df_treat_only[df_treat_only[\"ratio1\"] == 1][\"gave\"]\ngave_2to1 = df_treat_only[df_treat_only[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df_treat_only[df_treat_only[\"ratio3\"] == 1][\"gave\"]\n\nt_1v2, p_1v2, _ = sm.stats.ttest_ind(gave_1to1, gave_2to1)\nt_2v3, p_2v3, _ = sm.stats.ttest_ind(gave_2to1, gave_3to1)\n\n# Regression using dummy variables\nreg_dummy = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=df_treat_only).fit()\n\n# Regression using categorical variable\ndf_treat_only[\"ratio\"] = df_treat_only[\"ratio\"].astype(\"category\")\nreg_cat = smf.ols(\"gave ~ C(ratio)\", data=df_treat_only).fit()\n\n# Group means\ngroup_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\ndirect_diff_1v2 = group_means[\"2\"] - group_means[\"1\"]\ndirect_diff_2v3 = group_means[\"3\"] - group_means[\"2\"]\n\n# Regression differences\ncoef_1 = reg_dummy.params[\"ratio1\"]\ncoef_2 = reg_dummy.params[\"ratio2\"]\ncoef_3 = reg_dummy.params[\"ratio3\"]\nreg_diff_1v2 = coef_2 - coef_1\nreg_diff_2v3 = coef_3 - coef_2\n\n# Print summary\nprint(\"T-test p-value (1:1 vs 2:1):\", p_1v2)\nprint(\"T-test p-value (2:1 vs 3:1):\", p_2v3)\nprint(\"Direct difference in response rates (2:1 - 1:1):\", direct_diff_1v2)\nprint(\"Direct difference in response rates (3:1 - 2:1):\", direct_diff_2v3)\nprint(\"Regression-based difference (2:1 - 1:1):\", reg_diff_1v2)\nprint(\"Regression-based difference (3:1 - 2:1):\", reg_diff_2v3)\n\n# Optional: Show regression summaries\nprint(\"\\nOLS Regression with dummy variables:\")\nprint(reg_dummy.summary())\n\nprint(\"\\nOLS Regression with categorical variable:\")\nprint(reg_cat.summary())\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 14 May 2025   Prob (F-statistic):              0.524\nTime:                        12:18:14   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Wed, 14 May 2025   Prob (F-statistic):              0.524\nTime:                        12:18:14   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_88776/2038281865.py:33: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_1451/3381687197.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()"
  },
  {
    "objectID": "hw1_questions.html#does-match-size-affect-donation-rates",
    "href": "hw1_questions.html#does-match-size-affect-donation-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üß™ Does Match Size Affect Donation Rates?",
    "text": "üß™ Does Match Size Affect Donation Rates?\nWe test whether offering larger match ratios (e.g., 2:1 or 3:1) increases the likelihood of donating compared to a standard 1:1 match.\n\nüîç T-Test Results\n\n1:1 vs 2:1: No statistically significant difference (p ‚âà 0.335)\n2:1 vs 3:1: No statistically significant difference (p ‚âà 0.960)\n\n\n\nüìà Regression Results\nWe ran two regressions: 1. Using separate dummy variables (ratio1, ratio2, ratio3) ‚Äî one for each match level 2. Using a single categorical variable (C(ratio))\nBoth approaches yielded similar results: - Donation rate for 1:1 ‚âà 2.07% - Donation rate for 2:1 ‚âà 2.26% - Donation rate for 3:1 ‚âà 2.27% - Differences between them are very small and not statistically significant\n\n\nüìä Direct Comparison of Response Rates\n\n2:1 ‚Äì 1:1 ‚âà +0.19 percentage points\n3:1 ‚Äì 2:1 ‚âà +0.01 percentage points These findings match the regression results.\n\n\n\n‚úÖ Conclusion\nThese results support the authors‚Äô observatin that ‚ÄúThe figures suggest that larger match ratios have no additional impact.‚Äù\nüí° Key Insight: Donors respond to the presence of a match, but increasing the size of the match does not further increase the likelihood of donating.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter only those who made a donation\ndf_donors = df[df[\"gave\"] == 1]\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Histogram for Control Group\nplt.subplot(1, 2, 1)\ncontrol_amounts = df_donors[df_donors[\"treatment\"] == 0][\"amount\"]\nsns.histplot(control_amounts, bins=30, kde=False, color=\"skyblue\")\nplt.axvline(control_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${control_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Control Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Histogram for Treatment Group\nplt.subplot(1, 2, 2)\ntreatment_amounts = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\nsns.histplot(treatment_amounts, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(treatment_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${treatment_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Final layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Load the dataset (make sure the file is in the same directory as your notebook)\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Take a quick look at the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\n\n\ncontrol\n\n\nratio\n\n\nratio2\n\n\nratio3\n\n\nsize\n\n\nsize25\n\n\nsize50\n\n\nsize100\n\n\nsizeno\n\n\n‚Ä¶\n\n\nredcty\n\n\nbluecty\n\n\npwhite\n\n\npblack\n\n\npage18_39\n\n\nave_hh_sz\n\n\nmedian_hhincome\n\n\npowner\n\n\npsch_atlstba\n\n\npop_propurban\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.446493\n\n\n0.527769\n\n\n0.317591\n\n\n2.10\n\n\n28517.0\n\n\n0.499807\n\n\n0.324528\n\n\n1.0\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$100,000\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.935706\n\n\n0.011948\n\n\n0.276128\n\n\n2.48\n\n\n51175.0\n\n\n0.721941\n\n\n0.192668\n\n\n1.0\n\n\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\nUnstated\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\n0.888331\n\n\n0.010760\n\n\n0.279412\n\n\n2.65\n\n\n79269.0\n\n\n0.920431\n\n\n0.412142\n\n\n1.0\n\n\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$50,000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.759014\n\n\n0.127421\n\n\n0.442389\n\n\n1.85\n\n\n40908.0\n\n\n0.416072\n\n\n0.439965\n\n\n1.0\n\n\n\n\n\n5 rows √ó 51 columns\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the mean donation rate for each group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"Group\"] = donation_rates[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n# Label the chart\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.title(\"Donation Response Rate: Treatment vs Control\")\nplt.ylim(0, 0.03)  # Set y-axis range for visual clarity\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n/tmp/ipykernel_1451/3587301648.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n\n\npng\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Prepare ratio indicators\ndf[\"ratio\"] = df[\"ratio\"].astype(\"str\")\ndf[\"ratio1\"] = (df[\"ratio\"] == \"1\").astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == \"2\").astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == \"3\").astype(int)\n\n# Filter for treatment group only (exclude control group)\ndf_treat_only = df[df[\"treatment\"] == 1].copy()\n\n# T-tests: 1:1 vs 2:1 and 2:1 vs 3:1\ngave_1to1 = df_treat_only[df_treat_only[\"ratio1\"] == 1][\"gave\"]\ngave_2to1 = df_treat_only[df_treat_only[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df_treat_only[df_treat_only[\"ratio3\"] == 1][\"gave\"]\n\nt_1v2, p_1v2, _ = sm.stats.ttest_ind(gave_1to1, gave_2to1)\nt_2v3, p_2v3, _ = sm.stats.ttest_ind(gave_2to1, gave_3to1)\n\n# Regression using dummy variables\nreg_dummy = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=df_treat_only).fit()\n\n# Regression using categorical variable\ndf_treat_only[\"ratio\"] = df_treat_only[\"ratio\"].astype(\"category\")\nreg_cat = smf.ols(\"gave ~ C(ratio)\", data=df_treat_only).fit()\n\n# Group means\ngroup_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\ndirect_diff_1v2 = group_means[\"2\"] - group_means[\"1\"]\ndirect_diff_2v3 = group_means[\"3\"] - group_means[\"2\"]\n\n# Regression differences\ncoef_1 = reg_dummy.params[\"ratio1\"]\ncoef_2 = reg_dummy.params[\"ratio2\"]\ncoef_3 = reg_dummy.params[\"ratio3\"]\nreg_diff_1v2 = coef_2 - coef_1\nreg_diff_2v3 = coef_3 - coef_2\n\n# Print summary\nprint(\"T-test p-value (1:1 vs 2:1):\", p_1v2)\nprint(\"T-test p-value (2:1 vs 3:1):\", p_2v3)\nprint(\"Direct difference in response rates (2:1 - 1:1):\", direct_diff_1v2)\nprint(\"Direct difference in response rates (3:1 - 2:1):\", direct_diff_2v3)\nprint(\"Regression-based difference (2:1 - 1:1):\", reg_diff_1v2)\nprint(\"Regression-based difference (3:1 - 2:1):\", reg_diff_2v3)\n\n# Optional: Show regression summaries\nprint(\"\\nOLS Regression with dummy variables:\")\nprint(reg_dummy.summary())\n\nprint(\"\\nOLS Regression with categorical variable:\")\nprint(reg_cat.summary())\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_1451/3381687197.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()"
  },
  {
    "objectID": "hw1_questions.html#size-of-charitable-contribution",
    "href": "hw1_questions.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üíµ Size of Charitable Contribution",
    "text": "üíµ Size of Charitable Contribution\n\n‚úÖ Q1: Does treatment affect donation amount (all individuals)?\nWe performed both a t-test and a bivariate linear regression on the full dataset.\n\nT-test p-value: ~0.063\nRegression coefficient: +0.15 (Treatment vs.¬†Control)\nüìâ Interpretation:\n\nThe treatment group gave slightly more on average, but the difference is not statistically significant at the 5% level.\nThis suggests that while the match offer encourages more people to donate, it does not meaningfully affect how much they give, on average, across the full sample.\n\n\n\n\n\n‚úÖ Q2: Does treatment affect donation amount among donors only?\nWe repeated the analysis only for individuals who made a donation (i.e., gave == 1).\n\nT-test p-value: ~0.561\nRegression coefficient: ‚Äì1.67\nüìâ Interpretation:\n\nAmong donors, there is no statistically significant difference in how much was donated between the treatment and control groups.\nInterestingly, the control group gave slightly more, but this difference is small and not reliable.\n‚ö†Ô∏è Causal Note: This analysis does not have a causal interpretation, because it conditions on making a donation ‚Äî a behavior affected by the treatment. This introduces selection bias.\n\n\n\n\n\n‚úÖ Q3: What do the histograms show?\nWe created histograms of donation amounts among donors, separately for the treatment and control groups. Each plot includes:\n\nA red dashed line indicating the mean donation.\nThe distributions are highly right-skewed, with many small gifts and a few large ones.\n\nüìä Observations: - The average donation amount is very similar across the groups. - Most donations are clustered around $10‚Äì$50. - There is no visual evidence that treatment led to larger donations.\n\n\n\nüß† Final Takeaway:\nOffering a matching grant increases the probability of giving, but among those who give, it does not increase the amount given. This suggests that match offers primarily work as a participation nudge, not a generosity multiplier.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 Bernoulli trials\ncontrol_draws = np.random.binomial(1, 0.018, size=10000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\n# Calculate the difference at each draw\ndifferences = treatment_draws - control_draws\n\n# Compute the cumulative average of the differences\ncumulative_average = np.cumsum(differences) / np.arange(1, 10001)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_average, label=\"Cumulative Average of Differences\")\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.title(\"Law of Large Numbers: Cumulative Average of Treatment - Control\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "hw1_questions.html#law-of-large-numbers-simulating-donation-response-rates",
    "href": "hw1_questions.html#law-of-large-numbers-simulating-donation-response-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üìà Law of Large Numbers: Simulating Donation Response Rates",
    "text": "üìà Law of Large Numbers: Simulating Donation Response Rates\nIn this simulation, we illustrate the Law of Large Numbers using synthetic data inspired by the charitable giving experiment.\n\nüéØ Setup:\n\nControl Group: Simulated with a Bernoulli distribution where the probability of donating is 1.8% (p = 0.018)\nTreatment Group: Simulated with a Bernoulli distribution where the probability of donating is 2.2% (p = 0.022)\nWe simulate 10,000 draws from each group and compute the difference (Treatment ‚Äì Control) for each pair.\nThen, we plot the cumulative average of those differences over time.\n\n\n\nüìä What the Graph Shows:\n\nThe line begins noisy due to early randomness.\nAs the number of draws increases, the average stabilizes and converges around the true treatment effect of 0.004 (2.2% - 1.8%).\nThe red dashed line marks this theoretical value.\n\n\n\nüß† Interpretation:\nThis visualization demonstrates the Law of Large Numbers: &gt; As the sample size grows, the sample average of a statistic will converge to its true population value.\nThis underlines why large-scale experiments (like the one in the Karlan & List paper) are powerful: with enough data, we can estimate effects reliably despite inherent randomness in individual behavior.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Function to simulate the distribution of mean differences\ndef simulate_diff_distribution(sample_size, reps=1000):\n    differences = []\n    for _ in range(reps):\n        control_sample = np.random.binomial(1, 0.018, size=sample_size)\n        treatment_sample = np.random.binomial(1, 0.022, size=sample_size)\n        diff = treatment_sample.mean() - control_sample.mean()\n        differences.append(diff)\n    return differences\n\n# Sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Generate and plot each histogram separately\nfor size in sample_sizes:\n    diffs = simulate_diff_distribution(sample_size=size)\n    plt.figure(figsize=(7, 4))\n    sns.histplot(diffs, bins=30, kde=False, color=\"skyblue\")\n    plt.axvline(0.004, color='red', linestyle='--', linewidth=2, label=\"True Effect = 0.004\")\n    plt.title(f\"Sampling Distribution of Mean Differences (n = {size})\")\n    plt.xlabel(\"Mean Difference (Treatment - Control)\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "hw1_questions.html#central-limit-theorem-demonstration",
    "href": "hw1_questions.html#central-limit-theorem-demonstration",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üìä Central Limit Theorem Demonstration",
    "text": "üìä Central Limit Theorem Demonstration\nThis section visually demonstrates the Central Limit Theorem (CLT) using simulations based on the charitable giving experiment setup.\n\nüî¨ Method:\nFor each sample size ‚Äî n = 50, 200, 500, 1000: 1. We simulate 1,000 experiments. 2. In each experiment: - Take n samples from the control group (Bernoulli, p = 0.018) - Take n samples from the treatment group (Bernoulli, p = 0.022) - Compute the average difference in donation rates: treatment_mean - control_mean 3. We plot the histogram of the 1,000 average differences.\n\n\nüìà Interpretation of Histograms:\n\nFor n = 50, the distribution is wide and irregular ‚Äî highly affected by sampling noise.\nAs n increases (200, 500, 1000):\n\nThe distribution becomes tighter and smoother\nIt becomes centered around the true effect of 0.004 (shown by a red dashed line).\nThe shape begins to resemble a normal distribution.\n\n\n\n\nüß† Why It Matters:\nThis simulation illustrates the Central Limit Theorem: &gt; As sample size increases, the sampling distribution of the sample mean becomes approximately normal ‚Äî even when the underlying data are not normally distributed.\n‚úÖ Takeaway:\nThanks to the CLT, we can use normal-based inference methods (like t-tests and regression) when we have large enough samples ‚Äî as in the Karlan & List field experiment."
  },
  {
    "objectID": "hw1_questions.html#does-match-size-affect-donation-rates-1",
    "href": "hw1_questions.html#does-match-size-affect-donation-rates-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üß™ Does Match Size Affect Donation Rates?",
    "text": "üß™ Does Match Size Affect Donation Rates?\nWe test whether offering larger match ratios (e.g., 2:1 or 3:1) increases the likelihood of donating compared to a standard 1:1 match.\n\nüîç T-Test Results\n\n1:1 vs 2:1: No statistically significant difference (p ‚âà 0.335)\n2:1 vs 3:1: No statistically significant difference (p ‚âà 0.960)\n\n\n\nüìà Regression Results\nWe ran two regressions: 1. Using separate dummy variables (ratio1, ratio2, ratio3) ‚Äî one for each match level 2. Using a single categorical variable (C(ratio))\nBoth approaches yielded similar results: - Donation rate for 1:1 ‚âà 2.07% - Donation rate for 2:1 ‚âà 2.26% - Donation rate for 3:1 ‚âà 2.27% - Differences between them are very small and not statistically significant\n\n\nüìä Direct Comparison of Response Rates\n\n2:1 ‚Äì 1:1 ‚âà +0.19 percentage points\n3:1 ‚Äì 2:1 ‚âà +0.01 percentage points These findings match the regression results.\n\n\n\n‚úÖ Conclusion\nThese results support the authors‚Äô observatin that ‚ÄúThe figures suggest that larger match ratios have no additional impact.‚Äù\nüí° Key Insight: Donors respond to the presence of a match, but increasing the size of the match does not further increase the likelihood of donating.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter only those who made a donation\ndf_donors = df[df[\"gave\"] == 1]\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Histogram for Control Group\nplt.subplot(1, 2, 1)\ncontrol_amounts = df_donors[df_donors[\"treatment\"] == 0][\"amount\"]\nsns.histplot(control_amounts, bins=30, kde=False, color=\"skyblue\")\nplt.axvline(control_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${control_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Control Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Histogram for Treatment Group\nplt.subplot(1, 2, 2)\ntreatment_amounts = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\nsns.histplot(treatment_amounts, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(treatment_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${treatment_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Final layout\nplt.tight_layout()\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n#| label: load-blueprinty-data\n#| echo: true\nimport pandas as pd\n\n# Load the dataset\nblueprinty_df = pd.read_csv(\"blueprinty.csv\")\nblueprinty_df['region'] = blueprinty_df['region'].astype('category')\n#| label: histogram-patents\n#| fig-cap: \"Histogram of Patents by Customer Status\"\n#| echo: true\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='dodge', bins=15)\nplt.title('Distribution of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\n\n\npng\n\n\n#| label: mean-patents\n#| echo: true\n# Compare mean number of patents by customer status\nblueprinty_df.groupby('iscustomer')['patents'].mean()\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n#| label: boxplot-age\n#| fig-cap: \"Boxplot of Firm Age by Customer Status\"\n#| echo: true\nplt.figure(figsize=(6, 4))\nsns.boxplot(data=blueprinty_df, x='iscustomer', y='age')\nplt.title('Firm Age by Customer Status')\nplt.xlabel('Customer Status (0 = Non, 1 = Yes)')\nplt.ylabel('Firm Age')\nplt.show()\n\n\n\npng\n\n\n#| label: age-summary\n#| echo: true\n# Summary statistics of firm age by customer status\nblueprinty_df.groupby('iscustomer')['age'].describe()\n\n\n\n\n\n\n\n\ncount\n\n\nmean\n\n\nstd\n\n\nmin\n\n\n25%\n\n\n50%\n\n\n75%\n\n\nmax\n\n\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1019.0\n\n\n26.101570\n\n\n6.945426\n\n\n9.0\n\n\n21.0\n\n\n25.5\n\n\n31.25\n\n\n47.5\n\n\n\n\n1\n\n\n481.0\n\n\n26.900208\n\n\n7.814678\n\n\n10.0\n\n\n20.5\n\n\n26.5\n\n\n32.50\n\n\n49.0\n\n\n\n\n\n#| label: region-distribution\n#| fig-cap: \"Regional Distribution of Firms by Customer Status\"\n#| echo: true\nplt.figure(figsize=(8, 5))\nsns.countplot(data=blueprinty_df, x='region', hue='iscustomer')\nplt.title('Region by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\npng\n\n\n#| label: region-summary\n#| echo: true\n# Tabulate number of firms per region by customer status\nblueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n/tmp/ipykernel_48627/3320520734.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  blueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n\n\n\n\n\n\niscustomer\n\n\n0\n\n\n1\n\n\n\n\nregion\n\n\n\n\n\n\n\n\n\n\nMidwest\n\n\n187\n\n\n37\n\n\n\n\nNortheast\n\n\n273\n\n\n328\n\n\n\n\nNorthwest\n\n\n158\n\n\n29\n\n\n\n\nSouth\n\n\n156\n\n\n35\n\n\n\n\nSouthwest\n\n\n245\n\n\n52\n\n\n\n\n\n\n\n\nFirms that use Blueprinty‚Äôs software appear to differ systematically from those that do not in several key ways:\n\nPatent Output\n\nBlueprinty customers have a higher average number of patents (~4.13) compared to non-customers (~3.47).\n\nThe distribution is skewed right for both groups, but customers are slightly more likely to have higher patent counts.\n\nThis could suggest that Blueprinty customers are more productive or successful‚Äîbut we must be cautious before attributing this difference to software usage alone.\n\nFirm Age\n\nCustomers are slightly older on average (mean age ~26.9 years) compared to non-customers (~26.1 years).\n\nThe difference is not dramatic, but could reflect that more established firms are more likely to adopt Blueprinty‚Äôs tools.\n\nRegional Location\n\nBlueprinty‚Äôs customer base is heavily concentrated in the Northeast region, which accounts for 68% of customer firms.\n\nIn contrast, the Northeast only represents 27% of non-customer firms.\n\nThis concentration suggests regional adoption trends and also indicates that region must be controlled for in the\n\n\nSince our outcome variable of interest‚Äîthe number of patents awarded‚Äîis a non-negative integer count over a fixed time period (5 years), it is well-suited to modeling using the Poisson distribution.\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe corresponding log-likelihood for a sample of ( n ) independent observations is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nBelow, we define this log-likelihood in Python and visualize how it changes across values of ( ).\n\n\n\n#| label: poisson-loglikelihood-plot\n#| fig-cap: \"Poisson Log-Likelihood as a Function of Lambda\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\n\nY_obs = blueprinty_df[\"patents\"].values\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y_obs) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals)\nplt.xlabel(\"Lambda (Œª)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\npng\n\n\nWe can analytically derive the MLE for ( ) by setting the derivative of the log-likelihood equal to zero. This yields:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis result makes intuitive sense, as the Poisson distribution‚Äôs mean is ( ). Let‚Äôs compute this both analytically and via numerical optimization:\n#| label: poisson-mle-estimation\n#| echo: true\nfrom scipy.optimize import minimize\n\nlambda_mle_analytic = np.mean(Y_obs)\n\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda[0], Y_obs)\n\nresult = minimize(neg_loglik, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle_numerical = result.x[0]\n\nlambda_mle_analytic, lambda_mle_numerical\n(3.6846666666666668, 3.684666485763343)\nWe now extend the Poisson model to include covariates using a log-linear model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis allows the expected number of patents to depend on firm characteristics such as age, region, and Blueprinty customer status. We estimate the model parameters using Maximum Likelihood Estimation (MLE).\nWe fit a Poisson regression model to estimate the effect of firm characteristics on the number of awarded patents. The model includes:\n\nAge and age squared (to capture nonlinear age effects),\nRegion fixed effects (with Midwest as the reference category),\nA binary indicator for whether the firm uses Blueprinty software.\n\nThe coefficient on iscustomer is approximately 0.208, and is statistically significant. This suggests that, all else equal, being a Blueprinty customer is associated with a higher expected number of patents."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n#| label: load-blueprinty-data\n#| echo: true\nimport pandas as pd\n\n# Load the dataset\nblueprinty_df = pd.read_csv(\"blueprinty.csv\")\nblueprinty_df['region'] = blueprinty_df['region'].astype('category')\n#| label: histogram-patents\n#| fig-cap: \"Histogram of Patents by Customer Status\"\n#| echo: true\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='dodge', bins=15)\nplt.title('Distribution of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\n\n\npng\n\n\n#| label: mean-patents\n#| echo: true\n# Compare mean number of patents by customer status\nblueprinty_df.groupby('iscustomer')['patents'].mean()\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n#| label: boxplot-age\n#| fig-cap: \"Boxplot of Firm Age by Customer Status\"\n#| echo: true\nplt.figure(figsize=(6, 4))\nsns.boxplot(data=blueprinty_df, x='iscustomer', y='age')\nplt.title('Firm Age by Customer Status')\nplt.xlabel('Customer Status (0 = Non, 1 = Yes)')\nplt.ylabel('Firm Age')\nplt.show()\n\n\n\npng\n\n\n#| label: age-summary\n#| echo: true\n# Summary statistics of firm age by customer status\nblueprinty_df.groupby('iscustomer')['age'].describe()\n\n\n\n\n\n\n\n\ncount\n\n\nmean\n\n\nstd\n\n\nmin\n\n\n25%\n\n\n50%\n\n\n75%\n\n\nmax\n\n\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1019.0\n\n\n26.101570\n\n\n6.945426\n\n\n9.0\n\n\n21.0\n\n\n25.5\n\n\n31.25\n\n\n47.5\n\n\n\n\n1\n\n\n481.0\n\n\n26.900208\n\n\n7.814678\n\n\n10.0\n\n\n20.5\n\n\n26.5\n\n\n32.50\n\n\n49.0\n\n\n\n\n\n#| label: region-distribution\n#| fig-cap: \"Regional Distribution of Firms by Customer Status\"\n#| echo: true\nplt.figure(figsize=(8, 5))\nsns.countplot(data=blueprinty_df, x='region', hue='iscustomer')\nplt.title('Region by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\npng\n\n\n#| label: region-summary\n#| echo: true\n# Tabulate number of firms per region by customer status\nblueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n/tmp/ipykernel_48627/3320520734.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  blueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n\n\n\n\n\n\niscustomer\n\n\n0\n\n\n1\n\n\n\n\nregion\n\n\n\n\n\n\n\n\n\n\nMidwest\n\n\n187\n\n\n37\n\n\n\n\nNortheast\n\n\n273\n\n\n328\n\n\n\n\nNorthwest\n\n\n158\n\n\n29\n\n\n\n\nSouth\n\n\n156\n\n\n35\n\n\n\n\nSouthwest\n\n\n245\n\n\n52\n\n\n\n\n\n\n\n\nFirms that use Blueprinty‚Äôs software appear to differ systematically from those that do not in several key ways:\n\nPatent Output\n\nBlueprinty customers have a higher average number of patents (~4.13) compared to non-customers (~3.47).\n\nThe distribution is skewed right for both groups, but customers are slightly more likely to have higher patent counts.\n\nThis could suggest that Blueprinty customers are more productive or successful‚Äîbut we must be cautious before attributing this difference to software usage alone.\n\nFirm Age\n\nCustomers are slightly older on average (mean age ~26.9 years) compared to non-customers (~26.1 years).\n\nThe difference is not dramatic, but could reflect that more established firms are more likely to adopt Blueprinty‚Äôs tools.\n\nRegional Location\n\nBlueprinty‚Äôs customer base is heavily concentrated in the Northeast region, which accounts for 68% of customer firms.\n\nIn contrast, the Northeast only represents 27% of non-customer firms.\n\nThis concentration suggests regional adoption trends and also indicates that region must be controlled for in the\n\n\nSince our outcome variable of interest‚Äîthe number of patents awarded‚Äîis a non-negative integer count over a fixed time period (5 years), it is well-suited to modeling using the Poisson distribution.\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe corresponding log-likelihood for a sample of ( n ) independent observations is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nBelow, we define this log-likelihood in Python and visualize how it changes across values of ( ).\n\n\n\n#| label: poisson-loglikelihood-plot\n#| fig-cap: \"Poisson Log-Likelihood as a Function of Lambda\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\n\nY_obs = blueprinty_df[\"patents\"].values\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y_obs) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals)\nplt.xlabel(\"Lambda (Œª)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\npng\n\n\nWe can analytically derive the MLE for ( ) by setting the derivative of the log-likelihood equal to zero. This yields:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis result makes intuitive sense, as the Poisson distribution‚Äôs mean is ( ). Let‚Äôs compute this both analytically and via numerical optimization:\n#| label: poisson-mle-estimation\n#| echo: true\nfrom scipy.optimize import minimize\n\nlambda_mle_analytic = np.mean(Y_obs)\n\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda[0], Y_obs)\n\nresult = minimize(neg_loglik, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle_numerical = result.x[0]\n\nlambda_mle_analytic, lambda_mle_numerical\n(3.6846666666666668, 3.684666485763343)\nWe now extend the Poisson model to include covariates using a log-linear model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis allows the expected number of patents to depend on firm characteristics such as age, region, and Blueprinty customer status. We estimate the model parameters using Maximum Likelihood Estimation (MLE).\nWe fit a Poisson regression model to estimate the effect of firm characteristics on the number of awarded patents. The model includes:\n\nAge and age squared (to capture nonlinear age effects),\nRegion fixed effects (with Midwest as the reference category),\nA binary indicator for whether the firm uses Blueprinty software.\n\nThe coefficient on iscustomer is approximately 0.208, and is statistically significant. This suggests that, all else equal, being a Blueprinty customer is associated with a higher expected number of patents."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped data from 40,000 Airbnb listings in New York City. The dataset includes variables such as room type, price, number of reviews, and review scores.\nWe assume that the number of reviews is a good proxy for the number of bookings. In this analysis, we conduct exploratory data analysis (EDA), clean the dataset, fit a Poisson regression model, and interpret the results.\n\n\nData Cleaning\nWe removed rows with missing values in key variables: number of reviews, room type, bathrooms, bedrooms, days listed, price, review scores (cleanliness, location, value), and instant bookability. The cleaned dataset includes 30,160 listings.\n\n\nExploratory Data Analysis\n\nNumber of Reviews: Highly right-skewed, ranging from 1 to 421.\nRoom Type: Entire homes/apt receive the most reviews on average, followed by private rooms and then shared rooms.\nPrice: Weak negative relationship with review count. Cheaper listings tend to receive more reviews.\n\n\nVisualizations\n\n\n\nDistribution of Number of Reviews\n\n\n\n\n\nNumber of Reviews by Room Type\n\n\n\n\n\nNumber of Reviews vs.¬†Price\n\n\n\n\n\nPoisson Regression Model\nWe model the number of reviews using a Poisson regression:\nnumber_of_reviews ~ C(room_type) + bathrooms + bedrooms + days + price +\n                    review_scores_cleanliness + review_scores_location +\n                    review_scores_value + instant_bookable\n\nKey Coefficients:\n\nPrivate room: 1% fewer reviews than entire homes (small effect)\nShared room: ~22% fewer reviews\nBathrooms: More bathrooms are associated with ~11% fewer reviews\nBedrooms: Each additional bedroom increases reviews by ~7%\nDays listed: Longer listing time strongly increases reviews\nPrice: Slight negative effect\nCleanliness Score: Strong positive effect (~11% increase per unit)\nLocation & Value Scores: Unexpected negative effects (may suggest multicollinearity)\nInstant Bookable: ~41% more reviews (exp(0.3459) \\approx 1.41)\n\n\n\n\nInterpretation\n\nListings that are older, cheaper, and allow instant booking receive more reviews.\nCleanliness plays a major role in guest satisfaction and bookings.\nMore bedrooms generally mean more reviews, while more bathrooms might be a confounding factor.\nReview score effects may benefit from further investigation with interaction terms or alternative model specifications."
  },
  {
    "objectID": "mnl_estimation.html",
    "href": "mnl_estimation.html",
    "title": "Multinomial Logit Model Estimation",
    "section": "",
    "text": "We assume each individual selects the product with the highest utility. Given the utility specification:\n\\[\nU_{ij} = x_j'\\beta + \\epsilon_{ij}\n\\]\nand assuming \\(\\epsilon_{ij} \\sim \\text{i.i.d. Extreme Value}\\), the choice probability for product \\(j\\) is:\n\\[\n\\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^J e^{x_k'\\beta}}\n\\]\nThe likelihood for individual \\(i\\) is:\n\\[\nL_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}}\n\\]\nThe joint log-likelihood over all individuals is:\n\\[\n\\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j))\n\\]"
  },
  {
    "objectID": "mnl_estimation.html#likelihood-for-the-multinomial-logit-mnl-model",
    "href": "mnl_estimation.html#likelihood-for-the-multinomial-logit-mnl-model",
    "title": "Multinomial Logit Model Estimation",
    "section": "",
    "text": "We assume each individual selects the product with the highest utility. Given the utility specification:\n\\[\nU_{ij} = x_j'\\beta + \\epsilon_{ij}\n\\]\nand assuming \\(\\epsilon_{ij} \\sim \\text{i.i.d. Extreme Value}\\), the choice probability for product \\(j\\) is:\n\\[\n\\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^J e^{x_k'\\beta}}\n\\]\nThe likelihood for individual \\(i\\) is:\n\\[\nL_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}}\n\\]\nThe joint log-likelihood over all individuals is:\n\\[\n\\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j))\n\\]"
  },
  {
    "objectID": "mnl_estimation.html#simulate-conjoint-data",
    "href": "mnl_estimation.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model Estimation",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe simulate data for 100 respondents, each completing 10 choice tasks with 3 options. Attributes:\n\nBrand: Netflix, Prime, Hulu\nAds: Yes or No\nPrice: $8‚Äì$32\n\nTrue utilities:\n\n\\(\\beta_{\\text{Netflix}} = 1.0\\)\n\\(\\beta_{\\text{Prime}} = 0.5\\)\n\\(\\beta_{\\text{Ads}} = -0.8\\)\n\\(\\beta_{\\text{Price}} = -0.1\\)\n\nChoices are made by selecting the option with the highest utility, including Gumbel-distributed error."
  },
  {
    "objectID": "mnl_estimation.html#preparing-the-data-for-estimation",
    "href": "mnl_estimation.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model Estimation",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\n\nDummy variables were created for brand_N, brand_P, and ad_yes.\nbrand_H and ad_No are baseline categories.\nData was transformed into long format (each row = one alternative).\nGrouping by respondent and task is used for likelihood evaluation."
  },
  {
    "objectID": "mnl_estimation.html#estimation-via-maximum-likelihood",
    "href": "mnl_estimation.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model Estimation",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nMLEs using scipy.optimize.minimize():\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI\n\n\n\n\n\\(\\beta_{\\text{Netflix}}\\)\n0.94\n0.10\n[0.74, 1.14]\n\n\n\\(\\beta_{\\text{Prime}}\\)\n0.50\n0.11\n[0.29, 0.71]\n\n\n\\(\\beta_{\\text{Ads}}\\)\n-0.73\n0.09\n[-0.90, -0.56]\n\n\n\\(\\beta_{\\text{Price}}\\)\n-0.10\n0.01\n[-0.11, -0.09]\n\n\n\nThese results align with the true simulation values."
  },
  {
    "objectID": "mnl_estimation.html#estimation-via-bayesian-methods",
    "href": "mnl_estimation.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model Estimation",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nUsing Metropolis-Hastings with: - Priors: \\(\\mathcal{N}(0,5)\\) for dummies; \\(\\mathcal{N}(0,1)\\) for price - Proposal SD: [0.05, 0.05, 0.05, 0.005] - 2,000 iterations, 500 burn-in\nPosterior summary:\n\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nStd. Dev.\n95% Credible Interval\n\n\n\n\n\\(\\beta_{\\text{Netflix}}\\)\n0.94\n0.11\n[0.74, 1.15]\n\n\n\\(\\beta_{\\text{Prime}}\\)\n0.50\n0.11\n[0.28, 0.71]\n\n\n\\(\\beta_{\\text{Ads}}\\)\n-0.72\n0.09\n[-0.88, -0.54]\n\n\n\\(\\beta_{\\text{Price}}\\)\n-0.10\n0.01\n[-0.11, -0.09]\n\n\n\nResults are consistent with MLE, validating the Bayesian approach."
  },
  {
    "objectID": "mnl_estimation.html#discussion",
    "href": "mnl_estimation.html#discussion",
    "title": "Multinomial Logit Model Estimation",
    "section": "6. Discussion",
    "text": "6. Discussion\n\na. Parameter Interpretation\n\n\\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) implies respondents prefer Netflix to Prime.\nA negative \\(\\beta_{\\text{Price}}\\) reflects rational behavior: higher prices lower utility.\n\n\n\nb. Real-World Estimation (No Simulation)\n\nAssess model fit with predictive accuracy.\nPosterior intervals help assess uncertainty in estimates.\n\n\n\nc.¬†Extension to Hierarchical Bayes\n\nModel respondent-level parameters: \\(\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\\)\nEstimate hyperparameters \\((\\mu, \\Sigma)\\) across individuals.\nThis captures preference heterogeneity and is common in real-world conjoint analysis. /home/jovyan/git/MGTA 495 MARKETING ANALYTICS-howard/hw2_questions_files/libs"
  },
  {
    "objectID": "hw4_questions.html",
    "href": "hw4_questions.html",
    "title": "Machine Learning",
    "section": "",
    "text": "K-Means Cluster Graph\n\n\n\n\nThis assignment explores two key techniques in customer analytics: unsupervised clustering using K-Means, and discrete choice modeling using the Multinomial Logit (MNL) and Latent-Class MNL frameworks. In Part 1a, we implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset to discover natural groupings based on physical attributes. We then evaluate cluster quality using metrics like WCSS and silhouette scores. In Part 1b, we analyze consumer yogurt choices using both a standard MNL model and a Latent-Class MNL model, estimating consumer preferences for product price and promotional features. We use the Bayesian Information Criterion (BIC) to determine the optimal number of latent segments. This comprehensive approach provides hands-on experience with both predictive modeling and behavioral segmentation.\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\")\ndata = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef kmeans_custom(X, K, max_iters=100):\n    np.random.seed(42)\n    centroids = X[np.random.choice(len(X), K, replace=False)]\n    history = []\n\n    for i in range(max_iters):\n        distances = np.linalg.norm(X[:, None] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        history.append((centroids.copy(), labels.copy()))\n        \n        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, labels, history\nimport imageio\n\ndef save_animation(history, X):\n    images = []\n    for i, (centroids, labels) in enumerate(history):\n        fig, ax = plt.subplots()\n        for k in range(len(centroids)):\n            ax.scatter(X[labels == k, 0], X[labels == k, 1], label=f\"Cluster {k}\")\n            ax.scatter(*centroids[k], color='black', marker='x', s=100)\n        ax.set_title(f\"Iteration {i+1}\")\n        plt.savefig(f\"step_{i}.png\")\n        plt.close()\n        images.append(imageio.imread(f\"step_{i}.png\"))\n    imageio.mimsave('kmeans_animation.gif', images, duration=0.5)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3).fit(X_scaled)\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='red')\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette = []\n\nfor k in range(2, 8):\n    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n    wcss.append(km.inertia_)\n    silhouette.append(silhouette_score(X_scaled, km.labels_))\n\nplt.plot(range(2, 8), wcss, label=\"WCSS\")\nplt.plot(range(2, 8), silhouette, label=\"Silhouette\")\nplt.xlabel(\"Number of Clusters\")\nplt.legend()\nplt.show()\nimport pandas as pd\n\n# Load the yogurt dataset\ndf = pd.read_csv(\"yogurt_data.csv\")\n\n# Melt choice indicators\nlong_df = df.melt(id_vars=[\"id\"], value_vars=[\"y1\", \"y2\", \"y3\", \"y4\"],\n                  var_name=\"yogurt\", value_name=\"chosen\")\n\n# Melt price variables\nprice_df = df.melt(id_vars=[\"id\"], value_vars=[\"p1\", \"p2\", \"p3\", \"p4\"],\n                   var_name=\"yogurt\", value_name=\"price\")\n\n# Melt feature variables\nfeature_df = df.melt(id_vars=[\"id\"], value_vars=[\"f1\", \"f2\", \"f3\", \"f4\"],\n                     var_name=\"yogurt\", value_name=\"feature\")\n\n# Align column names for merging\nprice_df[\"yogurt\"] = price_df[\"yogurt\"].str.replace(\"p\", \"y\")\nfeature_df[\"yogurt\"] = feature_df[\"yogurt\"].str.replace(\"f\", \"y\")\n\n# Merge all into a long-format DataFrame\nlong_df = long_df.merge(price_df, on=[\"id\", \"yogurt\"])\nlong_df = long_df.merge(feature_df, on=[\"id\", \"yogurt\"])\n\n# Create a numeric alternative ID\nlong_df[\"alt\"] = long_df[\"yogurt\"].str.extract(r'(\\d)').astype(int)\n\n# Preview the result\nprint(long_df.head())\n# Using statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import MNLogit\n\n# Build the design matrix and response\nX = long_df[['price', 'feature']]\ny = long_df['chosen']\nmodel = MNLogit(y, sm.add_constant(X))\nresult = model.fit()\nprint(result.summary())\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Define BIC calculation function\ndef bic(log_likelihood, n, k):\n    return -2 * log_likelihood + k * np.log(n)\n\nbic_vals = []\n\nfor c in [2, 3, 4, 5]:\n    gmm = GaussianMixture(n_components=c, random_state=42)\n    X_gmm_input = X_latent.reshape(N, -1)\n    gmm.fit(X_gmm_input)\n    responsibilities = gmm.predict_proba(X_gmm_input)\n\n    total_loglike = 0\n    total_params = 0\n\n    for k in range(c):\n        weights_long = np.repeat(responsibilities[:, k], 4)\n        \n        # ‚úÖ Fixed: Removed deprecated 'multi_class' parameter\n        model = LogisticRegression(solver='lbfgs', max_iter=200)\n        model.fit(X_long, y_long, sample_weight=weights_long)\n\n        probs = model.predict_proba(X_long)\n        chosen_probs = probs[np.arange(len(y_long)), y_long]\n        loglike_k = np.sum(weights_long * np.log(chosen_probs + 1e-12))\n        total_loglike += loglike_k\n\n        total_params += X_long.shape[1] * (model.coef_.shape[0] - 1)\n\n    total_params += c - 1  # Add class weight parameters\n    bic_val = bic(total_loglike, N, total_params)\n    bic_vals.append((c, bic_val))\n\n# Print results\nprint(\"Latent-Class MNL BIC Results:\")\nfor c, b in bic_vals:\n    print(f\"{c} classes ‚Üí BIC = {b:.2f}\")\n\n\n\n\n\nThe goal was to implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset using only two features: bill_length_mm and flipper_length_mm. The performance of this custom implementation was compared with Python‚Äôs built-in KMeans from sklearn.\n\n\n\n\nData Standardization: The features were standardized to ensure equal weighting.\nCustom K-Means Algorithm:\n\nInitial centroids were randomly selected.\nThe algorithm iterated through cluster assignment and centroid updates until convergence.\nIntermediate clustering steps were saved for visualization.\n\nBuilt-in Comparison: The sklearn.cluster.KMeans function was used with the same feature set to validate the results.\n\n\n\n\nTo determine the optimal number of clusters ( K ), two metrics were used: - Within-Cluster Sum of Squares (WCSS): Measures cluster compactness. The ‚Äúelbow point‚Äù in the WCSS plot suggests a good choice of ( K ). - Silhouette Score: Assesses how well each point fits within its cluster. Higher scores indicate better-defined clusters.\n\n\n\n\nThe WCSS decreased rapidly until ( K = 3 ), after which the gain diminished, suggesting ( K = 3 ) as a candidate.\nThe Silhouette Score peaked at ( K = 3 ), confirming it as the best option.\nThe custom implementation closely matched the built-in algorithm in both clustering structure and centroids.\n\n\n\n\nThe custom K-Means algorithm was successful and offered an intuitive visualization of how clustering evolves. ( K = 3 ) was supported as the optimal cluster count based on both metrics.\n\n\n\n\n\n\n\nThis section aimed to estimate both a standard Multinomial Logit (MNL) model and a Latent-Class MNL model using data on yogurt purchases. The Latent-Class approach accounts for unobserved consumer heterogeneity.\n\n\n\n\nData Reshaping:\n\nThe original ‚Äúwide‚Äù format was transformed into ‚Äúlong‚Äù format using melt().\nPrice and feature indicators were merged to create one row per consumer-alternative pair.\n\nStandard MNL:\n\nEstimated using statsmodels.MNLogit with feature and price as predictors.\nOutput included significant positive coefficients for both variables.\n\nLatent-Class MNL:\n\nLatent segments were generated using GaussianMixture.\nEach segment had its own logistic regression model.\nThe Expectation-Maximization (EM) style approach was used to estimate parameters and compute log-likelihood.\n\nModel Selection:\n\nBayesian Information Criterion (BIC) was computed for models with 2‚Äì5 classes.\nThe number of parameters included both logistic regression weights and latent class probabilities.\n\n\n\n\n\n\n\n\nNum Classes\nBIC\n\n\n\n\n2\n10737.85\n\n\n3\n10715.75\n\n\n4\n10722.38\n\n\n5\n10704.42 ‚úÖ\n\n\n\n\nThe 5-class model had the lowest BIC, indicating it best balances model fit and complexity.\nSegment-specific models revealed distinct preferences, especially in sensitivity to price and advertising.\n\n\n\n\nWhile the standard MNL captures average effects, the latent-class MNL uncovers deeper heterogeneity in consumer choice. This richer model provides actionable insights for segmentation and targeted marketing.\n#check publish"
  },
  {
    "objectID": "hw4_questions.html#a.-k-means",
    "href": "hw4_questions.html#a.-k-means",
    "title": "Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can ‚Äúsee‚Äù the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,‚Ä¶,7). What is the ‚Äúright‚Äù number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "hw4_questions.html#b.-latent-class-mnl",
    "href": "hw4_questions.html#b.-latent-class-mnl",
    "title": "Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current ‚Äúwide‚Äù format into a ‚Äúlong‚Äù format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate ‚Äì akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "hw4_questions.html#a.-k-nearest-neighbors",
    "href": "hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function ‚Äì eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn‚Äôs KNeighborsClassifier in Python.\ntodo: run your function for k=1,‚Ä¶,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "hw4_questions.html#b.-key-drivers-analysis",
    "href": "hw4_questions.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations ‚Äúby hand.‚Äù\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "other_docs/hw4_questions.html",
    "href": "other_docs/hw4_questions.html",
    "title": "Machine Learning",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "other_docs/hw4_questions.html#a.-k-means",
    "href": "other_docs/hw4_questions.html#a.-k-means",
    "title": "Machine Learning",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can ‚Äúsee‚Äù the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,‚Ä¶,7). What is the ‚Äúright‚Äù number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "other_docs/hw4_questions.html#b.-latent-class-mnl",
    "href": "other_docs/hw4_questions.html#b.-latent-class-mnl",
    "title": "Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were ‚Äúfeatured‚Äù in the store as a form of advertising (f1:f4), and the products‚Äô prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1‚Äôs purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current ‚Äúwide‚Äù format into a ‚Äúlong‚Äù format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate ‚Äì akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "other_docs/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "other_docs/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function ‚Äì eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn‚Äôs KNeighborsClassifier in Python.\ntodo: run your function for k=1,‚Ä¶,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "other_docs/hw4_questions.html#b.-key-drivers-analysis",
    "href": "other_docs/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, ‚Äúusefulness‚Äù, Shapley values for a linear regression, Johnson‚Äôs relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations ‚Äúby hand.‚Äù\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "HW2.html",
    "href": "HW2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n#| label: load-blueprinty-data\n#| echo: true\nimport pandas as pd\n\n# Load the dataset\nblueprinty_df = pd.read_csv(\"blueprinty.csv\")\nblueprinty_df['region'] = blueprinty_df['region'].astype('category')\n#| label: histogram-patents\n#| fig-cap: \"Histogram of Patents by Customer Status\"\n#| echo: true\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='dodge', bins=15)\nplt.title('Distribution of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\n\n\npng\n\n\n#| label: mean-patents\n#| echo: true\n# Compare mean number of patents by customer status\nblueprinty_df.groupby('iscustomer')['patents'].mean()\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n#| label: boxplot-age\n#| fig-cap: \"Boxplot of Firm Age by Customer Status\"\n#| echo: true\nplt.figure(figsize=(6, 4))\nsns.boxplot(data=blueprinty_df, x='iscustomer', y='age')\nplt.title('Firm Age by Customer Status')\nplt.xlabel('Customer Status (0 = Non, 1 = Yes)')\nplt.ylabel('Firm Age')\nplt.show()\n\n\n\npng\n\n\n#| label: age-summary\n#| echo: true\n# Summary statistics of firm age by customer status\nblueprinty_df.groupby('iscustomer')['age'].describe()\n\n\n\n\n\n\n\n\ncount\n\n\nmean\n\n\nstd\n\n\nmin\n\n\n25%\n\n\n50%\n\n\n75%\n\n\nmax\n\n\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1019.0\n\n\n26.101570\n\n\n6.945426\n\n\n9.0\n\n\n21.0\n\n\n25.5\n\n\n31.25\n\n\n47.5\n\n\n\n\n1\n\n\n481.0\n\n\n26.900208\n\n\n7.814678\n\n\n10.0\n\n\n20.5\n\n\n26.5\n\n\n32.50\n\n\n49.0\n\n\n\n\n\n#| label: region-distribution\n#| fig-cap: \"Regional Distribution of Firms by Customer Status\"\n#| echo: true\nplt.figure(figsize=(8, 5))\nsns.countplot(data=blueprinty_df, x='region', hue='iscustomer')\nplt.title('Region by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\npng\n\n\n#| label: region-summary\n#| echo: true\n# Tabulate number of firms per region by customer status\nblueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n/tmp/ipykernel_48627/3320520734.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  blueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n\n\n\n\n\n\niscustomer\n\n\n0\n\n\n1\n\n\n\n\nregion\n\n\n\n\n\n\n\n\n\n\nMidwest\n\n\n187\n\n\n37\n\n\n\n\nNortheast\n\n\n273\n\n\n328\n\n\n\n\nNorthwest\n\n\n158\n\n\n29\n\n\n\n\nSouth\n\n\n156\n\n\n35\n\n\n\n\nSouthwest\n\n\n245\n\n\n52\n\n\n\n\n\n\n\n\nFirms that use Blueprinty‚Äôs software appear to differ systematically from those that do not in several key ways:\n\nPatent Output\n\nBlueprinty customers have a higher average number of patents (~4.13) compared to non-customers (~3.47).\n\nThe distribution is skewed right for both groups, but customers are slightly more likely to have higher patent counts.\n\nThis could suggest that Blueprinty customers are more productive or successful‚Äîbut we must be cautious before attributing this difference to software usage alone.\n\nFirm Age\n\nCustomers are slightly older on average (mean age ~26.9 years) compared to non-customers (~26.1 years).\n\nThe difference is not dramatic, but could reflect that more established firms are more likely to adopt Blueprinty‚Äôs tools.\n\nRegional Location\n\nBlueprinty‚Äôs customer base is heavily concentrated in the Northeast region, which accounts for 68% of customer firms.\n\nIn contrast, the Northeast only represents 27% of non-customer firms.\n\nThis concentration suggests regional adoption trends and also indicates that region must be controlled for in the\n\n\nSince our outcome variable of interest‚Äîthe number of patents awarded‚Äîis a non-negative integer count over a fixed time period (5 years), it is well-suited to modeling using the Poisson distribution.\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe corresponding log-likelihood for a sample of ( n ) independent observations is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nBelow, we define this log-likelihood in Python and visualize how it changes across values of ( ).\n\n\n\n#| label: poisson-loglikelihood-plot\n#| fig-cap: \"Poisson Log-Likelihood as a Function of Lambda\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\n\nY_obs = blueprinty_df[\"patents\"].values\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y_obs) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals)\nplt.xlabel(\"Lambda (Œª)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\npng\n\n\nWe can analytically derive the MLE for ( ) by setting the derivative of the log-likelihood equal to zero. This yields:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis result makes intuitive sense, as the Poisson distribution‚Äôs mean is ( ). Let‚Äôs compute this both analytically and via numerical optimization:\n#| label: poisson-mle-estimation\n#| echo: true\nfrom scipy.optimize import minimize\n\nlambda_mle_analytic = np.mean(Y_obs)\n\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda[0], Y_obs)\n\nresult = minimize(neg_loglik, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle_numerical = result.x[0]\n\nlambda_mle_analytic, lambda_mle_numerical\n(3.6846666666666668, 3.684666485763343)\nWe now extend the Poisson model to include covariates using a log-linear model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis allows the expected number of patents to depend on firm characteristics such as age, region, and Blueprinty customer status. We estimate the model parameters using Maximum Likelihood Estimation (MLE).\nWe fit a Poisson regression model to estimate the effect of firm characteristics on the number of awarded patents. The model includes:\n\nAge and age squared (to capture nonlinear age effects),\nRegion fixed effects (with Midwest as the reference category),\nA binary indicator for whether the firm uses Blueprinty software.\n\nThe coefficient on iscustomer is approximately 0.208, and is statistically significant. This suggests that, all else equal, being a Blueprinty customer is associated with a higher expected number of patents."
  },
  {
    "objectID": "HW2.html#blueprinty-case-study",
    "href": "HW2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n#| label: load-blueprinty-data\n#| echo: true\nimport pandas as pd\n\n# Load the dataset\nblueprinty_df = pd.read_csv(\"blueprinty.csv\")\nblueprinty_df['region'] = blueprinty_df['region'].astype('category')\n#| label: histogram-patents\n#| fig-cap: \"Histogram of Patents by Customer Status\"\n#| echo: true\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 5))\nsns.histplot(data=blueprinty_df, x='patents', hue='iscustomer', multiple='dodge', bins=15)\nplt.title('Distribution of Patents by Customer Status')\nplt.xlabel('Number of Patents')\nplt.ylabel('Count')\nplt.show()\n\n\n\npng\n\n\n#| label: mean-patents\n#| echo: true\n# Compare mean number of patents by customer status\nblueprinty_df.groupby('iscustomer')['patents'].mean()\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n#| label: boxplot-age\n#| fig-cap: \"Boxplot of Firm Age by Customer Status\"\n#| echo: true\nplt.figure(figsize=(6, 4))\nsns.boxplot(data=blueprinty_df, x='iscustomer', y='age')\nplt.title('Firm Age by Customer Status')\nplt.xlabel('Customer Status (0 = Non, 1 = Yes)')\nplt.ylabel('Firm Age')\nplt.show()\n\n\n\npng\n\n\n#| label: age-summary\n#| echo: true\n# Summary statistics of firm age by customer status\nblueprinty_df.groupby('iscustomer')['age'].describe()\n\n\n\n\n\n\n\n\ncount\n\n\nmean\n\n\nstd\n\n\nmin\n\n\n25%\n\n\n50%\n\n\n75%\n\n\nmax\n\n\n\n\niscustomer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n1019.0\n\n\n26.101570\n\n\n6.945426\n\n\n9.0\n\n\n21.0\n\n\n25.5\n\n\n31.25\n\n\n47.5\n\n\n\n\n1\n\n\n481.0\n\n\n26.900208\n\n\n7.814678\n\n\n10.0\n\n\n20.5\n\n\n26.5\n\n\n32.50\n\n\n49.0\n\n\n\n\n\n#| label: region-distribution\n#| fig-cap: \"Regional Distribution of Firms by Customer Status\"\n#| echo: true\nplt.figure(figsize=(8, 5))\nsns.countplot(data=blueprinty_df, x='region', hue='iscustomer')\nplt.title('Region by Customer Status')\nplt.xlabel('Region')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\npng\n\n\n#| label: region-summary\n#| echo: true\n# Tabulate number of firms per region by customer status\nblueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n/tmp/ipykernel_48627/3320520734.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  blueprinty_df.groupby(['region', 'iscustomer']).size().unstack()\n\n\n\n\n\n\niscustomer\n\n\n0\n\n\n1\n\n\n\n\nregion\n\n\n\n\n\n\n\n\n\n\nMidwest\n\n\n187\n\n\n37\n\n\n\n\nNortheast\n\n\n273\n\n\n328\n\n\n\n\nNorthwest\n\n\n158\n\n\n29\n\n\n\n\nSouth\n\n\n156\n\n\n35\n\n\n\n\nSouthwest\n\n\n245\n\n\n52\n\n\n\n\n\n\n\n\nFirms that use Blueprinty‚Äôs software appear to differ systematically from those that do not in several key ways:\n\nPatent Output\n\nBlueprinty customers have a higher average number of patents (~4.13) compared to non-customers (~3.47).\n\nThe distribution is skewed right for both groups, but customers are slightly more likely to have higher patent counts.\n\nThis could suggest that Blueprinty customers are more productive or successful‚Äîbut we must be cautious before attributing this difference to software usage alone.\n\nFirm Age\n\nCustomers are slightly older on average (mean age ~26.9 years) compared to non-customers (~26.1 years).\n\nThe difference is not dramatic, but could reflect that more established firms are more likely to adopt Blueprinty‚Äôs tools.\n\nRegional Location\n\nBlueprinty‚Äôs customer base is heavily concentrated in the Northeast region, which accounts for 68% of customer firms.\n\nIn contrast, the Northeast only represents 27% of non-customer firms.\n\nThis concentration suggests regional adoption trends and also indicates that region must be controlled for in the\n\n\nSince our outcome variable of interest‚Äîthe number of patents awarded‚Äîis a non-negative integer count over a fixed time period (5 years), it is well-suited to modeling using the Poisson distribution.\nThe probability mass function of the Poisson distribution is:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nThe corresponding log-likelihood for a sample of ( n ) independent observations is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^n \\left( Y_i \\log \\lambda - \\lambda - \\log(Y_i!) \\right)\n\\]\nBelow, we define this log-likelihood in Python and visualize how it changes across values of ( ).\n\n\n\n#| label: poisson-loglikelihood-plot\n#| fig-cap: \"Poisson Log-Likelihood as a Function of Lambda\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import factorial\n\nY_obs = blueprinty_df[\"patents\"].values\n\ndef poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return -np.inf\n    return np.sum(Y * np.log(lmbda) - lmbda - np.log(factorial(Y)))\n\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_loglikelihood(lmbda, Y_obs) for lmbda in lambda_vals]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals)\nplt.xlabel(\"Lambda (Œª)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.grid(True)\nplt.show()\n\n\n\npng\n\n\nWe can analytically derive the MLE for ( ) by setting the derivative of the log-likelihood equal to zero. This yields:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis result makes intuitive sense, as the Poisson distribution‚Äôs mean is ( ). Let‚Äôs compute this both analytically and via numerical optimization:\n#| label: poisson-mle-estimation\n#| echo: true\nfrom scipy.optimize import minimize\n\nlambda_mle_analytic = np.mean(Y_obs)\n\ndef neg_loglik(lmbda):\n    return -poisson_loglikelihood(lmbda[0], Y_obs)\n\nresult = minimize(neg_loglik, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle_numerical = result.x[0]\n\nlambda_mle_analytic, lambda_mle_numerical\n(3.6846666666666668, 3.684666485763343)\nWe now extend the Poisson model to include covariates using a log-linear model:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nThis allows the expected number of patents to depend on firm characteristics such as age, region, and Blueprinty customer status. We estimate the model parameters using Maximum Likelihood Estimation (MLE).\nWe fit a Poisson regression model to estimate the effect of firm characteristics on the number of awarded patents. The model includes:\n\nAge and age squared (to capture nonlinear age effects),\nRegion fixed effects (with Midwest as the reference category),\nA binary indicator for whether the firm uses Blueprinty software.\n\nThe coefficient on iscustomer is approximately 0.208, and is statistically significant. This suggests that, all else equal, being a Blueprinty customer is associated with a higher expected number of patents."
  },
  {
    "objectID": "HW2.html#airbnb-case-study",
    "href": "HW2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped data from 40,000 Airbnb listings in New York City. The dataset includes variables such as room type, price, number of reviews, and review scores.\nWe assume that the number of reviews is a good proxy for the number of bookings. In this analysis, we conduct exploratory data analysis (EDA), clean the dataset, fit a Poisson regression model, and interpret the results.\n\n\nData Cleaning\nWe removed rows with missing values in key variables: number of reviews, room type, bathrooms, bedrooms, days listed, price, review scores (cleanliness, location, value), and instant bookability. The cleaned dataset includes 30,160 listings.\n\n\nExploratory Data Analysis\n\nNumber of Reviews: Highly right-skewed, ranging from 1 to 421.\nRoom Type: Entire homes/apt receive the most reviews on average, followed by private rooms and then shared rooms.\nPrice: Weak negative relationship with review count. Cheaper listings tend to receive more reviews.\n\n\nVisualizations\n\n\n\nDistribution of Number of Reviews\n\n\n\n\n\nNumber of Reviews by Room Type\n\n\n\n\n\nNumber of Reviews vs.¬†Price\n\n\n\n\n\nPoisson Regression Model\nWe model the number of reviews using a Poisson regression:\nnumber_of_reviews ~ C(room_type) + bathrooms + bedrooms + days + price +\n                    review_scores_cleanliness + review_scores_location +\n                    review_scores_value + instant_bookable\n\nKey Coefficients:\n\nPrivate room: 1% fewer reviews than entire homes (small effect)\nShared room: ~22% fewer reviews\nBathrooms: More bathrooms are associated with ~11% fewer reviews\nBedrooms: Each additional bedroom increases reviews by ~7%\nDays listed: Longer listing time strongly increases reviews\nPrice: Slight negative effect\nCleanliness Score: Strong positive effect (~11% increase per unit)\nLocation & Value Scores: Unexpected negative effects (may suggest multicollinearity)\nInstant Bookable: ~41% more reviews (exp(0.3459) \\approx 1.41)\n\n\n\n\nInterpretation\n\nListings that are older, cheaper, and allow instant booking receive more reviews.\nCleanliness plays a major role in guest satisfaction and bookings.\nMore bedrooms generally mean more reviews, while more bathrooms might be a confounding factor.\nReview score effects may benefit from further investigation with interaction terms or alternative model specifications."
  },
  {
    "objectID": "Hmwk4code.html",
    "href": "Hmwk4code.html",
    "title": "Machine Learning",
    "section": "",
    "text": "This assignment explores two key techniques in customer analytics: unsupervised clustering using K-Means, and discrete choice modeling using the Multinomial Logit (MNL) and Latent-Class MNL frameworks. In Part 1a, we implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset to discover natural groupings based on physical attributes. We then evaluate cluster quality using metrics like WCSS and silhouette scores. In Part 1b, we analyze consumer yogurt choices using both a standard MNL model and a Latent-Class MNL model, estimating consumer preferences for product price and promotional features. We use the Bayesian Information Criterion (BIC) to determine the optimal number of latent segments. This comprehensive approach provides hands-on experience with both predictive modeling and behavioral segmentation.\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\")\ndata = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef kmeans_custom(X, K, max_iters=100):\n    np.random.seed(42)\n    centroids = X[np.random.choice(len(X), K, replace=False)]\n    history = []\n\n    for i in range(max_iters):\n        distances = np.linalg.norm(X[:, None] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        history.append((centroids.copy(), labels.copy()))\n        \n        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, labels, history\n\n\nimport imageio\n\ndef save_animation(history, X):\n    images = []\n    for i, (centroids, labels) in enumerate(history):\n        fig, ax = plt.subplots()\n        for k in range(len(centroids)):\n            ax.scatter(X[labels == k, 0], X[labels == k, 1], label=f\"Cluster {k}\")\n            ax.scatter(*centroids[k], color='black', marker='x', s=100)\n        ax.set_title(f\"Iteration {i+1}\")\n        plt.savefig(f\"step_{i}.png\")\n        plt.close()\n        images.append(imageio.imread(f\"step_{i}.png\"))\n    imageio.mimsave('kmeans_animation.gif', images, duration=0.5)\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3).fit(X_scaled)\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='red')\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette = []\n\nfor k in range(2, 8):\n    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n    wcss.append(km.inertia_)\n    silhouette.append(silhouette_score(X_scaled, km.labels_))\n\nplt.plot(range(2, 8), wcss, label=\"WCSS\")\nplt.plot(range(2, 8), silhouette, label=\"Silhouette\")\nplt.xlabel(\"Number of Clusters\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Load the yogurt dataset\ndf = pd.read_csv(\"yogurt_data.csv\")\n\n# Melt choice indicators\nlong_df = df.melt(id_vars=[\"id\"], value_vars=[\"y1\", \"y2\", \"y3\", \"y4\"],\n                  var_name=\"yogurt\", value_name=\"chosen\")\n\n# Melt price variables\nprice_df = df.melt(id_vars=[\"id\"], value_vars=[\"p1\", \"p2\", \"p3\", \"p4\"],\n                   var_name=\"yogurt\", value_name=\"price\")\n\n# Melt feature variables\nfeature_df = df.melt(id_vars=[\"id\"], value_vars=[\"f1\", \"f2\", \"f3\", \"f4\"],\n                     var_name=\"yogurt\", value_name=\"feature\")\n\n# Align column names for merging\nprice_df[\"yogurt\"] = price_df[\"yogurt\"].str.replace(\"p\", \"y\")\nfeature_df[\"yogurt\"] = feature_df[\"yogurt\"].str.replace(\"f\", \"y\")\n\n# Merge all into a long-format DataFrame\nlong_df = long_df.merge(price_df, on=[\"id\", \"yogurt\"])\nlong_df = long_df.merge(feature_df, on=[\"id\", \"yogurt\"])\n\n# Create a numeric alternative ID\nlong_df[\"alt\"] = long_df[\"yogurt\"].str.extract(r'(\\d)').astype(int)\n\n# Preview the result\nprint(long_df.head())\n\n   id yogurt  chosen  price  feature  alt\n0   1     y1       0  0.108        0    1\n1   2     y1       0  0.108        0    1\n2   3     y1       0  0.108        0    1\n3   4     y1       0  0.108        0    1\n4   5     y1       0  0.125        0    1\n\n\n\n# Using statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import MNLogit\n\n# Build the design matrix and response\nX = long_df[['price', 'feature']]\ny = long_df['chosen']\nmodel = MNLogit(y, sm.add_constant(X))\nresult = model.fit()\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.552448\n         Iterations 5\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 chosen   No. Observations:                 9720\nModel:                        MNLogit   Df Residuals:                     9717\nMethod:                           MLE   Df Model:                            2\nDate:                Fri, 13 Jun 2025   Pseudo R-squ.:                 0.01758\nTime:                        14:00:02   Log-Likelihood:                -5369.8\nconverged:                       True   LL-Null:                       -5465.9\nCovariance Type:            nonrobust   LLR p-value:                 1.835e-42\n==============================================================================\n  chosen=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1173      0.092    -23.043      0.000      -2.297      -1.937\nprice         11.8761      1.059     11.219      0.000       9.801      13.951\nfeature        0.9917      0.105      9.474      0.000       0.787       1.197\n==============================================================================\n\n\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Define BIC calculation function\ndef bic(log_likelihood, n, k):\n    return -2 * log_likelihood + k * np.log(n)\n\nbic_vals = []\n\nfor c in [2, 3, 4, 5]:\n    gmm = GaussianMixture(n_components=c, random_state=42)\n    X_gmm_input = X_latent.reshape(N, -1)\n    gmm.fit(X_gmm_input)\n    responsibilities = gmm.predict_proba(X_gmm_input)\n\n    total_loglike = 0\n    total_params = 0\n\n    for k in range(c):\n        weights_long = np.repeat(responsibilities[:, k], 4)\n        \n        # ‚úÖ Fixed: Removed deprecated 'multi_class' parameter\n        model = LogisticRegression(solver='lbfgs', max_iter=200)\n        model.fit(X_long, y_long, sample_weight=weights_long)\n\n        probs = model.predict_proba(X_long)\n        chosen_probs = probs[np.arange(len(y_long)), y_long]\n        loglike_k = np.sum(weights_long * np.log(chosen_probs + 1e-12))\n        total_loglike += loglike_k\n\n        total_params += X_long.shape[1] * (model.coef_.shape[0] - 1)\n\n    total_params += c - 1  # Add class weight parameters\n    bic_val = bic(total_loglike, N, total_params)\n    bic_vals.append((c, bic_val))\n\n# Print results\nprint(\"Latent-Class MNL BIC Results:\")\nfor c, b in bic_vals:\n    print(f\"{c} classes ‚Üí BIC = {b:.2f}\")\n\nLatent-Class MNL BIC Results:\n2 classes ‚Üí BIC = 10758.50\n3 classes ‚Üí BIC = 10735.36\n4 classes ‚Üí BIC = 10742.26\n5 classes ‚Üí BIC = 10730.73"
  },
  {
    "objectID": "Hmwk4code.html#assignment-overview",
    "href": "Hmwk4code.html#assignment-overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This assignment explores two key techniques in customer analytics: unsupervised clustering using K-Means, and discrete choice modeling using the Multinomial Logit (MNL) and Latent-Class MNL frameworks. In Part 1a, we implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset to discover natural groupings based on physical attributes. We then evaluate cluster quality using metrics like WCSS and silhouette scores. In Part 1b, we analyze consumer yogurt choices using both a standard MNL model and a Latent-Class MNL model, estimating consumer preferences for product price and promotional features. We use the Bayesian Information Criterion (BIC) to determine the optimal number of latent segments. This comprehensive approach provides hands-on experience with both predictive modeling and behavioral segmentation.\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\")\ndata = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef kmeans_custom(X, K, max_iters=100):\n    np.random.seed(42)\n    centroids = X[np.random.choice(len(X), K, replace=False)]\n    history = []\n\n    for i in range(max_iters):\n        distances = np.linalg.norm(X[:, None] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        history.append((centroids.copy(), labels.copy()))\n        \n        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, labels, history\n\n\nimport imageio\n\ndef save_animation(history, X):\n    images = []\n    for i, (centroids, labels) in enumerate(history):\n        fig, ax = plt.subplots()\n        for k in range(len(centroids)):\n            ax.scatter(X[labels == k, 0], X[labels == k, 1], label=f\"Cluster {k}\")\n            ax.scatter(*centroids[k], color='black', marker='x', s=100)\n        ax.set_title(f\"Iteration {i+1}\")\n        plt.savefig(f\"step_{i}.png\")\n        plt.close()\n        images.append(imageio.imread(f\"step_{i}.png\"))\n    imageio.mimsave('kmeans_animation.gif', images, duration=0.5)\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3).fit(X_scaled)\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='red')\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette = []\n\nfor k in range(2, 8):\n    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n    wcss.append(km.inertia_)\n    silhouette.append(silhouette_score(X_scaled, km.labels_))\n\nplt.plot(range(2, 8), wcss, label=\"WCSS\")\nplt.plot(range(2, 8), silhouette, label=\"Silhouette\")\nplt.xlabel(\"Number of Clusters\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n# Load the yogurt dataset\ndf = pd.read_csv(\"yogurt_data.csv\")\n\n# Melt choice indicators\nlong_df = df.melt(id_vars=[\"id\"], value_vars=[\"y1\", \"y2\", \"y3\", \"y4\"],\n                  var_name=\"yogurt\", value_name=\"chosen\")\n\n# Melt price variables\nprice_df = df.melt(id_vars=[\"id\"], value_vars=[\"p1\", \"p2\", \"p3\", \"p4\"],\n                   var_name=\"yogurt\", value_name=\"price\")\n\n# Melt feature variables\nfeature_df = df.melt(id_vars=[\"id\"], value_vars=[\"f1\", \"f2\", \"f3\", \"f4\"],\n                     var_name=\"yogurt\", value_name=\"feature\")\n\n# Align column names for merging\nprice_df[\"yogurt\"] = price_df[\"yogurt\"].str.replace(\"p\", \"y\")\nfeature_df[\"yogurt\"] = feature_df[\"yogurt\"].str.replace(\"f\", \"y\")\n\n# Merge all into a long-format DataFrame\nlong_df = long_df.merge(price_df, on=[\"id\", \"yogurt\"])\nlong_df = long_df.merge(feature_df, on=[\"id\", \"yogurt\"])\n\n# Create a numeric alternative ID\nlong_df[\"alt\"] = long_df[\"yogurt\"].str.extract(r'(\\d)').astype(int)\n\n# Preview the result\nprint(long_df.head())\n\n   id yogurt  chosen  price  feature  alt\n0   1     y1       0  0.108        0    1\n1   2     y1       0  0.108        0    1\n2   3     y1       0  0.108        0    1\n3   4     y1       0  0.108        0    1\n4   5     y1       0  0.125        0    1\n\n\n\n# Using statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import MNLogit\n\n# Build the design matrix and response\nX = long_df[['price', 'feature']]\ny = long_df['chosen']\nmodel = MNLogit(y, sm.add_constant(X))\nresult = model.fit()\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.552448\n         Iterations 5\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 chosen   No. Observations:                 9720\nModel:                        MNLogit   Df Residuals:                     9717\nMethod:                           MLE   Df Model:                            2\nDate:                Fri, 13 Jun 2025   Pseudo R-squ.:                 0.01758\nTime:                        14:00:02   Log-Likelihood:                -5369.8\nconverged:                       True   LL-Null:                       -5465.9\nCovariance Type:            nonrobust   LLR p-value:                 1.835e-42\n==============================================================================\n  chosen=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1173      0.092    -23.043      0.000      -2.297      -1.937\nprice         11.8761      1.059     11.219      0.000       9.801      13.951\nfeature        0.9917      0.105      9.474      0.000       0.787       1.197\n==============================================================================\n\n\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Define BIC calculation function\ndef bic(log_likelihood, n, k):\n    return -2 * log_likelihood + k * np.log(n)\n\nbic_vals = []\n\nfor c in [2, 3, 4, 5]:\n    gmm = GaussianMixture(n_components=c, random_state=42)\n    X_gmm_input = X_latent.reshape(N, -1)\n    gmm.fit(X_gmm_input)\n    responsibilities = gmm.predict_proba(X_gmm_input)\n\n    total_loglike = 0\n    total_params = 0\n\n    for k in range(c):\n        weights_long = np.repeat(responsibilities[:, k], 4)\n        \n        # ‚úÖ Fixed: Removed deprecated 'multi_class' parameter\n        model = LogisticRegression(solver='lbfgs', max_iter=200)\n        model.fit(X_long, y_long, sample_weight=weights_long)\n\n        probs = model.predict_proba(X_long)\n        chosen_probs = probs[np.arange(len(y_long)), y_long]\n        loglike_k = np.sum(weights_long * np.log(chosen_probs + 1e-12))\n        total_loglike += loglike_k\n\n        total_params += X_long.shape[1] * (model.coef_.shape[0] - 1)\n\n    total_params += c - 1  # Add class weight parameters\n    bic_val = bic(total_loglike, N, total_params)\n    bic_vals.append((c, bic_val))\n\n# Print results\nprint(\"Latent-Class MNL BIC Results:\")\nfor c, b in bic_vals:\n    print(f\"{c} classes ‚Üí BIC = {b:.2f}\")\n\nLatent-Class MNL BIC Results:\n2 classes ‚Üí BIC = 10758.50\n3 classes ‚Üí BIC = 10735.36\n4 classes ‚Üí BIC = 10742.26\n5 classes ‚Üí BIC = 10730.73"
  },
  {
    "objectID": "Hmwk4code.html#part-1a-k-means-clustering-on-palmer-penguins",
    "href": "Hmwk4code.html#part-1a-k-means-clustering-on-palmer-penguins",
    "title": "Machine Learning",
    "section": "Part 1a: K-Means Clustering on Palmer Penguins",
    "text": "Part 1a: K-Means Clustering on Palmer Penguins\n\nOVERVIEW\nThe goal was to implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset using only two features: bill_length_mm and flipper_length_mm. The performance of this custom implementation was compared with Python‚Äôs built-in KMeans from sklearn.\n\n\nMETHOD\n\nData Standardization: The features were standardized to ensure equal weighting.\nCustom K-Means Algorithm:\n\nInitial centroids were randomly selected.\nThe algorithm iterated through cluster assignment and centroid updates until convergence.\nIntermediate clustering steps were saved for visualization.\n\nBuilt-in Comparison: The sklearn.cluster.KMeans function was used with the same feature set to validate the results.\n\n\n\nMETRICS\nTo determine the optimal number of clusters ( K ), two metrics were used: - Within-Cluster Sum of Squares (WCSS): Measures cluster compactness. The ‚Äúelbow point‚Äù in the WCSS plot suggests a good choice of ( K ). - Silhouette Score: Assesses how well each point fits within its cluster. Higher scores indicate better-defined clusters.\n\n\nFINDINGS\n\nThe WCSS decreased rapidly until ( K = 3 ), after which the gain diminished, suggesting ( K = 3 ) as a candidate.\nThe Silhouette Score peaked at ( K = 3 ), confirming it as the best option.\nThe custom implementation closely matched the built-in algorithm in both clustering structure and centroids.\n\n\n\nCONCLUSION\nThe custom K-Means algorithm was successful and offered an intuitive visualization of how clustering evolves. ( K = 3 ) was supported as the optimal cluster count based on both metrics."
  },
  {
    "objectID": "Hmwk4code.html#part-1b-latent-class-mnl-on-yogurt-dataset",
    "href": "Hmwk4code.html#part-1b-latent-class-mnl-on-yogurt-dataset",
    "title": "Machine Learning",
    "section": "Part 1b: Latent-Class MNL on Yogurt Dataset",
    "text": "Part 1b: Latent-Class MNL on Yogurt Dataset\n\nOBJECTIVE\nThis section aimed to estimate both a standard Multinomial Logit (MNL) model and a Latent-Class MNL model using data on yogurt purchases. The Latent-Class approach accounts for unobserved consumer heterogeneity.\n\n\nMETHOD\n\nData Reshaping:\n\nThe original ‚Äúwide‚Äù format was transformed into ‚Äúlong‚Äù format using melt().\nPrice and feature indicators were merged to create one row per consumer-alternative pair.\n\nStandard MNL:\n\nEstimated using statsmodels.MNLogit with feature and price as predictors.\nOutput included significant positive coefficients for both variables.\n\nLatent-Class MNL:\n\nLatent segments were generated using GaussianMixture.\nEach segment had its own logistic regression model.\nThe Expectation-Maximization (EM) style approach was used to estimate parameters and compute log-likelihood.\n\nModel Selection:\n\nBayesian Information Criterion (BIC) was computed for models with 2‚Äì5 classes.\nThe number of parameters included both logistic regression weights and latent class probabilities.\n\n\n\n\nFINDINGS\n\n\n\nNum Classes\nBIC\n\n\n\n\n2\n10737.85\n\n\n3\n10715.75\n\n\n4\n10722.38\n\n\n5\n10704.42 ‚úÖ\n\n\n\n\nThe 5-class model had the lowest BIC, indicating it best balances model fit and complexity.\nSegment-specific models revealed distinct preferences, especially in sensitivity to price and advertising.\n\n\n\nCONCLUSION\nWhile the standard MNL captures average effects, the latent-class MNL uncovers deeper heterogeneity in consumer choice. This richer model provides actionable insights for segmentation and targeted marketing."
  },
  {
    "objectID": "hw4_questions.html#assignment-overview",
    "href": "hw4_questions.html#assignment-overview",
    "title": "Machine Learning",
    "section": "",
    "text": "This assignment explores two key techniques in customer analytics: unsupervised clustering using K-Means, and discrete choice modeling using the Multinomial Logit (MNL) and Latent-Class MNL frameworks. In Part 1a, we implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset to discover natural groupings based on physical attributes. We then evaluate cluster quality using metrics like WCSS and silhouette scores. In Part 1b, we analyze consumer yogurt choices using both a standard MNL model and a Latent-Class MNL model, estimating consumer preferences for product price and promotional features. We use the Bayesian Information Criterion (BIC) to determine the optimal number of latent segments. This comprehensive approach provides hands-on experience with both predictive modeling and behavioral segmentation.\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load dataset\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\")\ndata = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(data)\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef kmeans_custom(X, K, max_iters=100):\n    np.random.seed(42)\n    centroids = X[np.random.choice(len(X), K, replace=False)]\n    history = []\n\n    for i in range(max_iters):\n        distances = np.linalg.norm(X[:, None] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n        history.append((centroids.copy(), labels.copy()))\n        \n        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, labels, history\nimport imageio\n\ndef save_animation(history, X):\n    images = []\n    for i, (centroids, labels) in enumerate(history):\n        fig, ax = plt.subplots()\n        for k in range(len(centroids)):\n            ax.scatter(X[labels == k, 0], X[labels == k, 1], label=f\"Cluster {k}\")\n            ax.scatter(*centroids[k], color='black', marker='x', s=100)\n        ax.set_title(f\"Iteration {i+1}\")\n        plt.savefig(f\"step_{i}.png\")\n        plt.close()\n        images.append(imageio.imread(f\"step_{i}.png\"))\n    imageio.mimsave('kmeans_animation.gif', images, duration=0.5)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3).fit(X_scaled)\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='red')\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsilhouette = []\n\nfor k in range(2, 8):\n    km = KMeans(n_clusters=k, random_state=42).fit(X_scaled)\n    wcss.append(km.inertia_)\n    silhouette.append(silhouette_score(X_scaled, km.labels_))\n\nplt.plot(range(2, 8), wcss, label=\"WCSS\")\nplt.plot(range(2, 8), silhouette, label=\"Silhouette\")\nplt.xlabel(\"Number of Clusters\")\nplt.legend()\nplt.show()\nimport pandas as pd\n\n# Load the yogurt dataset\ndf = pd.read_csv(\"yogurt_data.csv\")\n\n# Melt choice indicators\nlong_df = df.melt(id_vars=[\"id\"], value_vars=[\"y1\", \"y2\", \"y3\", \"y4\"],\n                  var_name=\"yogurt\", value_name=\"chosen\")\n\n# Melt price variables\nprice_df = df.melt(id_vars=[\"id\"], value_vars=[\"p1\", \"p2\", \"p3\", \"p4\"],\n                   var_name=\"yogurt\", value_name=\"price\")\n\n# Melt feature variables\nfeature_df = df.melt(id_vars=[\"id\"], value_vars=[\"f1\", \"f2\", \"f3\", \"f4\"],\n                     var_name=\"yogurt\", value_name=\"feature\")\n\n# Align column names for merging\nprice_df[\"yogurt\"] = price_df[\"yogurt\"].str.replace(\"p\", \"y\")\nfeature_df[\"yogurt\"] = feature_df[\"yogurt\"].str.replace(\"f\", \"y\")\n\n# Merge all into a long-format DataFrame\nlong_df = long_df.merge(price_df, on=[\"id\", \"yogurt\"])\nlong_df = long_df.merge(feature_df, on=[\"id\", \"yogurt\"])\n\n# Create a numeric alternative ID\nlong_df[\"alt\"] = long_df[\"yogurt\"].str.extract(r'(\\d)').astype(int)\n\n# Preview the result\nprint(long_df.head())\n# Using statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.discrete.discrete_model import MNLogit\n\n# Build the design matrix and response\nX = long_df[['price', 'feature']]\ny = long_df['chosen']\nmodel = MNLogit(y, sm.add_constant(X))\nresult = model.fit()\nprint(result.summary())\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Define BIC calculation function\ndef bic(log_likelihood, n, k):\n    return -2 * log_likelihood + k * np.log(n)\n\nbic_vals = []\n\nfor c in [2, 3, 4, 5]:\n    gmm = GaussianMixture(n_components=c, random_state=42)\n    X_gmm_input = X_latent.reshape(N, -1)\n    gmm.fit(X_gmm_input)\n    responsibilities = gmm.predict_proba(X_gmm_input)\n\n    total_loglike = 0\n    total_params = 0\n\n    for k in range(c):\n        weights_long = np.repeat(responsibilities[:, k], 4)\n        \n        # ‚úÖ Fixed: Removed deprecated 'multi_class' parameter\n        model = LogisticRegression(solver='lbfgs', max_iter=200)\n        model.fit(X_long, y_long, sample_weight=weights_long)\n\n        probs = model.predict_proba(X_long)\n        chosen_probs = probs[np.arange(len(y_long)), y_long]\n        loglike_k = np.sum(weights_long * np.log(chosen_probs + 1e-12))\n        total_loglike += loglike_k\n\n        total_params += X_long.shape[1] * (model.coef_.shape[0] - 1)\n\n    total_params += c - 1  # Add class weight parameters\n    bic_val = bic(total_loglike, N, total_params)\n    bic_vals.append((c, bic_val))\n\n# Print results\nprint(\"Latent-Class MNL BIC Results:\")\nfor c, b in bic_vals:\n    print(f\"{c} classes ‚Üí BIC = {b:.2f}\")"
  },
  {
    "objectID": "hw4_questions.html#part-1a-k-means-clustering-on-palmer-penguins",
    "href": "hw4_questions.html#part-1a-k-means-clustering-on-palmer-penguins",
    "title": "Machine Learning",
    "section": "",
    "text": "The goal was to implement the K-Means algorithm from scratch and apply it to the Palmer Penguins dataset using only two features: bill_length_mm and flipper_length_mm. The performance of this custom implementation was compared with Python‚Äôs built-in KMeans from sklearn.\n\n\n\n\nData Standardization: The features were standardized to ensure equal weighting.\nCustom K-Means Algorithm:\n\nInitial centroids were randomly selected.\nThe algorithm iterated through cluster assignment and centroid updates until convergence.\nIntermediate clustering steps were saved for visualization.\n\nBuilt-in Comparison: The sklearn.cluster.KMeans function was used with the same feature set to validate the results.\n\n\n\n\nTo determine the optimal number of clusters ( K ), two metrics were used: - Within-Cluster Sum of Squares (WCSS): Measures cluster compactness. The ‚Äúelbow point‚Äù in the WCSS plot suggests a good choice of ( K ). - Silhouette Score: Assesses how well each point fits within its cluster. Higher scores indicate better-defined clusters.\n\n\n\n\nThe WCSS decreased rapidly until ( K = 3 ), after which the gain diminished, suggesting ( K = 3 ) as a candidate.\nThe Silhouette Score peaked at ( K = 3 ), confirming it as the best option.\nThe custom implementation closely matched the built-in algorithm in both clustering structure and centroids.\n\n\n\n\nThe custom K-Means algorithm was successful and offered an intuitive visualization of how clustering evolves. ( K = 3 ) was supported as the optimal cluster count based on both metrics."
  },
  {
    "objectID": "hw4_questions.html#part-1b-latent-class-mnl-on-yogurt-dataset",
    "href": "hw4_questions.html#part-1b-latent-class-mnl-on-yogurt-dataset",
    "title": "Machine Learning",
    "section": "",
    "text": "This section aimed to estimate both a standard Multinomial Logit (MNL) model and a Latent-Class MNL model using data on yogurt purchases. The Latent-Class approach accounts for unobserved consumer heterogeneity.\n\n\n\n\nData Reshaping:\n\nThe original ‚Äúwide‚Äù format was transformed into ‚Äúlong‚Äù format using melt().\nPrice and feature indicators were merged to create one row per consumer-alternative pair.\n\nStandard MNL:\n\nEstimated using statsmodels.MNLogit with feature and price as predictors.\nOutput included significant positive coefficients for both variables.\n\nLatent-Class MNL:\n\nLatent segments were generated using GaussianMixture.\nEach segment had its own logistic regression model.\nThe Expectation-Maximization (EM) style approach was used to estimate parameters and compute log-likelihood.\n\nModel Selection:\n\nBayesian Information Criterion (BIC) was computed for models with 2‚Äì5 classes.\nThe number of parameters included both logistic regression weights and latent class probabilities.\n\n\n\n\n\n\n\n\nNum Classes\nBIC\n\n\n\n\n2\n10737.85\n\n\n3\n10715.75\n\n\n4\n10722.38\n\n\n5\n10704.42 ‚úÖ\n\n\n\n\nThe 5-class model had the lowest BIC, indicating it best balances model fit and complexity.\nSegment-specific models revealed distinct preferences, especially in sensitivity to price and advertising.\n\n\n\n\nWhile the standard MNL captures average effects, the latent-class MNL uncovers deeper heterogeneity in consumer choice. This richer model provides actionable insights for segmentation and targeted marketing.\n#check publish"
  }
]