[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sarah Howard",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Howard Website HW1",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard‚Äôs Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription"
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic ‚Äúworks,‚Äù in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Simulate 10,000 draws from the control distribution and 10,000 draws from the treatment distribution. You‚Äôll then calculate a vector of 10,000 differences, and then you‚Äôll plot the cumulative average of that vector of differences. This average will likely be ‚Äúnoisey‚Äù when only averaging a few numbers, but should ‚Äúsettle down‚Äù and approximate the treatment effect (0.004 = 0.022 - 0.018) as the sample size gets large. Explain the chart to the reader.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms at sample sizes 50, 200, 500, and 1000. To do this for a sample size of e.g.¬†50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. The repeat for the other 3 histograms. Explain this sequence of histograms and its relationship to the central limit theorem to the reader."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Sarah Howard",
    "section": "",
    "text": "title: ‚ÄúResume‚Äù format: hmtl: default typst: default Download PDF file. Welcome to my website!"
  },
  {
    "objectID": "hw1_questions.html#overview",
    "href": "hw1_questions.html#overview",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Overview",
    "text": "Overview\nThis dataset contains data from a large-scale natural field experiment investigating how different types of matching grants influence charitable giving.\n\nTotal Observations: 50,083 individuals\n\nVariables: 51\n\nSource: Does Price Matter in Charitable Giving? (Karlan & List, 2007)"
  },
  {
    "objectID": "hw1_questions.html#experimental-design-variables",
    "href": "hw1_questions.html#experimental-design-variables",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üéØ Experimental Design Variables",
    "text": "üéØ Experimental Design Variables\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\n1 if participant received a matching grant offer, 0 if control group\n\n\nratio\nCategorical: Match ratio (e.g., 1:1, 2:1, 3:1)\n\n\nratio2, ratio3\nDummies for 2:1 and 3:1 match conditions\n\n\nsize\nCategorical: Maximum match size ($25k, $50k, $100k, or unstated)\n\n\nsize25, size50, size100, sizeno\nDummy variables for match size\n\n\nask1, ask2, ask3\nSuggested donation amounts (based on prior giving)"
  },
  {
    "objectID": "hw1_questions.html#donation-behavior-variables",
    "href": "hw1_questions.html#donation-behavior-variables",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üí∏ Donation Behavior Variables",
    "text": "üí∏ Donation Behavior Variables\n\n\n\nVariable\nDescription\n\n\n\n\ngave\nBinary: 1 if donated, 0 otherwise\n\n\namount\nAmount donated (USD)\n\n\namountchange\nChange in amount donated vs.¬†previous gift"
  },
  {
    "objectID": "hw1_questions.html#demographic-zip-code-level-data",
    "href": "hw1_questions.html#demographic-zip-code-level-data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üßë‚Äçü§ù‚Äçüßë Demographic & ZIP Code-level Data",
    "text": "üßë‚Äçü§ù‚Äçüßë Demographic & ZIP Code-level Data\n\n\n\nVariable\nDescription\n\n\n\n\npwhite, pblack\nProportion of white and Black residents\n\n\npage18_39\nProportion aged 18‚Äì39\n\n\nave_hh_sz\nAverage household size\n\n\nmedian_hhincome\nMedian household income\n\n\npowner\nProportion of homeowners\n\n\npsch_atlstba\nProportion with at least a bachelor‚Äôs degree\n\n\npop_propurban\nProportion of population in urban areas"
  },
  {
    "objectID": "hw1_questions.html#political-context-variables",
    "href": "hw1_questions.html#political-context-variables",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üó≥Ô∏è Political Context Variables",
    "text": "üó≥Ô∏è Political Context Variables\n\n\n\nVariable\nDescription\n\n\n\n\nred0, blue0\nBinary: Red or blue state indicator\n\n\nredcty, bluecty\nBinary: Red or blue county indicator"
  },
  {
    "objectID": "hw1_questions.html#missing-data",
    "href": "hw1_questions.html#missing-data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "‚ùó Missing Data",
    "text": "‚ùó Missing Data\nSome variables (especially demographic ones) have missing values for ~2,000 cases due to ZIP-level data availability.\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\nBalance Test\n\n\n‚úÖ Randomization Check: Treatment vs.¬†Control Group Balance\nTo test the integrity of the random assignment, we compared several background variables between treatment and control groups using t-tests.\n\nüîç Variables Tested:\n\nmrm2: Months since last donation\n\nyears: Years since first donation\n\nhpa: Highest prior donation\n\nfemale: Female indicator\n\ncouple: Couple indicator\n\n\n\nüìà Key Findings:\n\nNo statistically significant differences (p &gt; 0.05) were found between groups for any variable tested.\nFor mrm2, both a t-test and a linear regression (mrm2 ~ treatment) produced the same result (p = 0.905), confirming the methods agree.\n\n\n\nüß† Why It Matters:\nThese tests confirm that the treatment and control groups were statistically similar before the intervention ‚Äî supporting the internal validity of the experiment.\nThis is the purpose of Table 1 in the paper: to demonstrate that any differences in donation behavior can be attributed to the matching grant treatment, not pre-existing group differences.\n\n\n\nExperimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\nüìä Response Rate by Treatment Group\nWe visualize whether being offered a matching donation affects the likelihood of donating. This barplot compares the donation response rate between the treatment and control groups.\n\nimport pandas as pd\n\n# Load the dataset (make sure the file is in the same directory as your notebook)\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Take a quick look at the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\n\n\ntreatment\n\n\ncontrol\n\n\nratio\n\n\nratio2\n\n\nratio3\n\n\nsize\n\n\nsize25\n\n\nsize50\n\n\nsize100\n\n\nsizeno\n\n\n‚Ä¶\n\n\nredcty\n\n\nbluecty\n\n\npwhite\n\n\npblack\n\n\npage18_39\n\n\nave_hh_sz\n\n\nmedian_hhincome\n\n\npowner\n\n\npsch_atlstba\n\n\npop_propurban\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.446493\n\n\n0.527769\n\n\n0.317591\n\n\n2.10\n\n\n28517.0\n\n\n0.499807\n\n\n0.324528\n\n\n1.0\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$100,000\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.935706\n\n\n0.011948\n\n\n0.276128\n\n\n2.48\n\n\n51175.0\n\n\n0.721941\n\n\n0.192668\n\n\n1.0\n\n\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\nUnstated\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\n0.888331\n\n\n0.010760\n\n\n0.279412\n\n\n2.65\n\n\n79269.0\n\n\n0.920431\n\n\n0.412142\n\n\n1.0\n\n\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$50,000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.759014\n\n\n0.127421\n\n\n0.442389\n\n\n1.85\n\n\n40908.0\n\n\n0.416072\n\n\n0.439965\n\n\n1.0\n\n\n\n\n\n5 rows √ó 51 columns\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the mean donation rate for each group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"Group\"] = donation_rates[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n# Label the chart\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.title(\"Donation Response Rate: Treatment vs Control\")\nplt.ylim(0, 0.03)  # Set y-axis range for visual clarity\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n/tmp/ipykernel_66588/1362242253.py:10: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1451/3587301648.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n\n\npng\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Prepare ratio indicators\ndf[\"ratio\"] = df[\"ratio\"].astype(\"str\")\ndf[\"ratio1\"] = (df[\"ratio\"] == \"1\").astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == \"2\").astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == \"3\").astype(int)\n\n# Filter for treatment group only (exclude control group)\ndf_treat_only = df[df[\"treatment\"] == 1].copy()\n\n# T-tests: 1:1 vs 2:1 and 2:1 vs 3:1\ngave_1to1 = df_treat_only[df_treat_only[\"ratio1\"] == 1][\"gave\"]\ngave_2to1 = df_treat_only[df_treat_only[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df_treat_only[df_treat_only[\"ratio3\"] == 1][\"gave\"]\n\nt_1v2, p_1v2, _ = sm.stats.ttest_ind(gave_1to1, gave_2to1)\nt_2v3, p_2v3, _ = sm.stats.ttest_ind(gave_2to1, gave_3to1)\n\n# Regression using dummy variables\nreg_dummy = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=df_treat_only).fit()\n\n# Regression using categorical variable\ndf_treat_only[\"ratio\"] = df_treat_only[\"ratio\"].astype(\"category\")\nreg_cat = smf.ols(\"gave ~ C(ratio)\", data=df_treat_only).fit()\n\n# Group means\ngroup_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\ndirect_diff_1v2 = group_means[\"2\"] - group_means[\"1\"]\ndirect_diff_2v3 = group_means[\"3\"] - group_means[\"2\"]\n\n# Regression differences\ncoef_1 = reg_dummy.params[\"ratio1\"]\ncoef_2 = reg_dummy.params[\"ratio2\"]\ncoef_3 = reg_dummy.params[\"ratio3\"]\nreg_diff_1v2 = coef_2 - coef_1\nreg_diff_2v3 = coef_3 - coef_2\n\n# Print summary\nprint(\"T-test p-value (1:1 vs 2:1):\", p_1v2)\nprint(\"T-test p-value (2:1 vs 3:1):\", p_2v3)\nprint(\"Direct difference in response rates (2:1 - 1:1):\", direct_diff_1v2)\nprint(\"Direct difference in response rates (3:1 - 2:1):\", direct_diff_2v3)\nprint(\"Regression-based difference (2:1 - 1:1):\", reg_diff_1v2)\nprint(\"Regression-based difference (3:1 - 2:1):\", reg_diff_2v3)\n\n# Optional: Show regression summaries\nprint(\"\\nOLS Regression with dummy variables:\")\nprint(reg_dummy.summary())\n\nprint(\"\\nOLS Regression with categorical variable:\")\nprint(reg_cat.summary())\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        03:03:32   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        03:03:32   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_66588/2038281865.py:33: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_1451/3381687197.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\n\n\nüß™ Does Match Size Affect Donation Rates?\nWe test whether offering larger match ratios (e.g., 2:1 or 3:1) increases the likelihood of donating compared to a standard 1:1 match.\n\nüîç T-Test Results\n\n1:1 vs 2:1: No statistically significant difference (p ‚âà 0.335)\n2:1 vs 3:1: No statistically significant difference (p ‚âà 0.960)\n\n\n\nüìà Regression Results\nWe ran two regressions: 1. Using separate dummy variables (ratio1, ratio2, ratio3) ‚Äî one for each match level 2. Using a single categorical variable (C(ratio))\nBoth approaches yielded similar results: - Donation rate for 1:1 ‚âà 2.07% - Donation rate for 2:1 ‚âà 2.26% - Donation rate for 3:1 ‚âà 2.27% - Differences between them are very small and not statistically significant\n\n\nüìä Direct Comparison of Response Rates\n\n2:1 ‚Äì 1:1 ‚âà +0.19 percentage points\n3:1 ‚Äì 2:1 ‚âà +0.01 percentage points These findings match the regression results.\n\n\n\n‚úÖ Conclusion\nThese results support the authors‚Äô observatin that ‚ÄúThe figures suggest that larger match ratios have no additional impact.‚Äù\nüí° Key Insight: Donors respond to the presence of a match, but increasing the size of the match does not further increase the likelihood of donating.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter only those who made a donation\ndf_donors = df[df[\"gave\"] == 1]\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Histogram for Control Group\nplt.subplot(1, 2, 1)\ncontrol_amounts = df_donors[df_donors[\"treatment\"] == 0][\"amount\"]\nsns.histplot(control_amounts, bins=30, kde=False, color=\"skyblue\")\nplt.axvline(control_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${control_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Control Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Histogram for Treatment Group\nplt.subplot(1, 2, 2)\ntreatment_amounts = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\nsns.histplot(treatment_amounts, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(treatment_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${treatment_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Final layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npng\n\n\n\n\n\nüíµ Size of Charitable Contribution\n\n‚úÖ Q1: Does treatment affect donation amount (all individuals)?\nWe performed both a t-test and a bivariate linear regression on the full dataset.\n\nT-test p-value: ~0.063\nRegression coefficient: +0.15 (Treatment vs.¬†Control)\nüìâ Interpretation:\n\nThe treatment group gave slightly more on average, but the difference is not statistically significant at the 5% level.\nThis suggests that while the match offer encourages more people to donate, it does not meaningfully affect how much they give, on average, across the full sample.\n\n\n\n\n\n‚úÖ Q2: Does treatment affect donation amount among donors only?\nWe repeated the analysis only for individuals who made a donation (i.e., gave == 1).\n\nT-test p-value: ~0.561\nRegression coefficient: ‚Äì1.67\nüìâ Interpretation:\n\nAmong donors, there is no statistically significant difference in how much was donated between the treatment and control groups.\nInterestingly, the control group gave slightly more, but this difference is small and not reliable.\n‚ö†Ô∏è Causal Note: This analysis does not have a causal interpretation, because it conditions on making a donation ‚Äî a behavior affected by the treatment. This introduces selection bias.\n\n\n\n\n\n‚úÖ Q3: What do the histograms show?\nWe created histograms of donation amounts among donors, separately for the treatment and control groups. Each plot includes:\n\nA red dashed line indicating the mean donation.\nThe distributions are highly right-skewed, with many small gifts and a few large ones.\n\nüìä Observations: - The average donation amount is very similar across the groups. - Most donations are clustered around $10‚Äì$50. - There is no visual evidence that treatment led to larger donations.\n\n\n\nüß† Final Takeaway:\nOffering a matching grant increases the probability of giving, but among those who give, it does not increase the amount given. This suggests that match offers primarily work as a participation nudge, not a generosity multiplier.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 Bernoulli trials\ncontrol_draws = np.random.binomial(1, 0.018, size=10000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\n# Calculate the difference at each draw\ndifferences = treatment_draws - control_draws\n\n# Compute the cumulative average of the differences\ncumulative_average = np.cumsum(differences) / np.arange(1, 10001)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_average, label=\"Cumulative Average of Differences\")\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.title(\"Law of Large Numbers: Cumulative Average of Treatment - Control\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npng\n\n\n\n\n\nüìà Law of Large Numbers: Simulating Donation Response Rates\nIn this simulation, we illustrate the Law of Large Numbers using synthetic data inspired by the charitable giving experiment.\n\nüéØ Setup:\n\nControl Group: Simulated with a Bernoulli distribution where the probability of donating is 1.8% (p = 0.018)\nTreatment Group: Simulated with a Bernoulli distribution where the probability of donating is 2.2% (p = 0.022)\nWe simulate 10,000 draws from each group and compute the difference (Treatment ‚Äì Control) for each pair.\nThen, we plot the cumulative average of those differences over time.\n\n\n\nüìä What the Graph Shows:\n\nThe line begins noisy due to early randomness.\nAs the number of draws increases, the average stabilizes and converges around the true treatment effect of 0.004 (2.2% - 1.8%).\nThe red dashed line marks this theoretical value.\n\n\n\nüß† Interpretation:\nThis visualization demonstrates the Law of Large Numbers: &gt; As the sample size grows, the sample average of a statistic will converge to its true population value.\nThis underlines why large-scale experiments (like the one in the Karlan & List paper) are powerful: with enough data, we can estimate effects reliably despite inherent randomness in individual behavior.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Function to simulate the distribution of mean differences\ndef simulate_diff_distribution(sample_size, reps=1000):\n    differences = []\n    for _ in range(reps):\n        control_sample = np.random.binomial(1, 0.018, size=sample_size)\n        treatment_sample = np.random.binomial(1, 0.022, size=sample_size)\n        diff = treatment_sample.mean() - control_sample.mean()\n        differences.append(diff)\n    return differences\n\n# Sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Generate and plot each histogram separately\nfor size in sample_sizes:\n    diffs = simulate_diff_distribution(sample_size=size)\n    plt.figure(figsize=(7, 4))\n    sns.histplot(diffs, bins=30, kde=False, color=\"skyblue\")\n    plt.axvline(0.004, color='red', linestyle='--', linewidth=2, label=\"True Effect = 0.004\")\n    plt.title(f\"Sampling Distribution of Mean Differences (n = {size})\")\n    plt.xlabel(\"Mean Difference (Treatment - Control)\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\nüìä Central Limit Theorem Demonstration\nThis section visually demonstrates the Central Limit Theorem (CLT) using simulations based on the charitable giving experiment setup.\n\nüî¨ Method:\nFor each sample size ‚Äî n = 50, 200, 500, 1000: 1. We simulate 1,000 experiments. 2. In each experiment: - Take n samples from the control group (Bernoulli, p = 0.018) - Take n samples from the treatment group (Bernoulli, p = 0.022) - Compute the average difference in donation rates: treatment_mean - control_mean 3. We plot the histogram of the 1,000 average differences.\n\n\nüìà Interpretation of Histograms:\n\nFor n = 50, the distribution is wide and irregular ‚Äî highly affected by sampling noise.\nAs n increases (200, 500, 1000):\n\nThe distribution becomes tighter and smoother\nIt becomes centered around the true effect of 0.004 (shown by a red dashed line).\nThe shape begins to resemble a normal distribution.\n\n\n\n\nüß† Why It Matters:\nThis simulation illustrates the Central Limit Theorem: &gt; As sample size increases, the sampling distribution of the sample mean becomes approximately normal ‚Äî even when the underlying data are not normally distributed.\n‚úÖ Takeaway:\nThanks to the CLT, we can use normal-based inference methods (like t-tests and regression) when we have large enough samples ‚Äî as in the Karlan & List field experiment."
  },
  {
    "objectID": "hw1_questions.html#randomization-check-treatment-vs.-control-group-balance",
    "href": "hw1_questions.html#randomization-check-treatment-vs.-control-group-balance",
    "title": "A Replication of Karlan and List (2007)",
    "section": "‚úÖ Randomization Check: Treatment vs.¬†Control Group Balance",
    "text": "‚úÖ Randomization Check: Treatment vs.¬†Control Group Balance\nTo test the integrity of the random assignment, we compared several background variables between treatment and control groups using t-tests.\n\nüîç Variables Tested:\n\nmrm2: Months since last donation\n\nyears: Years since first donation\n\nhpa: Highest prior donation\n\nfemale: Female indicator\n\ncouple: Couple indicator\n\n\n\nüìà Key Findings:\n\nNo statistically significant differences (p &gt; 0.05) were found between groups for any variable tested.\nFor mrm2, both a t-test and a linear regression (mrm2 ~ treatment) produced the same result (p = 0.905), confirming the methods agree.\n\n\n\nüß† Why It Matters:\nThese tests confirm that the treatment and control groups were statistically similar before the intervention ‚Äî supporting the internal validity of the experiment.\nThis is the purpose of Table 1 in the paper: to demonstrate that any differences in donation behavior can be attributed to the matching grant treatment, not pre-existing group differences."
  },
  {
    "objectID": "hw1_questions.html#response-rate-by-treatment-group",
    "href": "hw1_questions.html#response-rate-by-treatment-group",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üìä Response Rate by Treatment Group",
    "text": "üìä Response Rate by Treatment Group\nWe visualize whether being offered a matching donation affects the likelihood of donating. This barplot compares the donation response rate between the treatment and control groups.\n\nimport pandas as pd\n\n# Load the dataset (make sure the file is in the same directory as your notebook)\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Take a quick look at the first few rows\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows √ó 51 columns\n\n\n\n\n\n\n\n\n\n\n\ntreatment\n\n\ncontrol\n\n\nratio\n\n\nratio2\n\n\nratio3\n\n\nsize\n\n\nsize25\n\n\nsize50\n\n\nsize100\n\n\nsizeno\n\n\n‚Ä¶\n\n\nredcty\n\n\nbluecty\n\n\npwhite\n\n\npblack\n\n\npage18_39\n\n\nave_hh_sz\n\n\nmedian_hhincome\n\n\npowner\n\n\npsch_atlstba\n\n\npop_propurban\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.446493\n\n\n0.527769\n\n\n0.317591\n\n\n2.10\n\n\n28517.0\n\n\n0.499807\n\n\n0.324528\n\n\n1.0\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\nControl\n\n\n0\n\n\n0\n\n\nControl\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$100,000\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.935706\n\n\n0.011948\n\n\n0.276128\n\n\n2.48\n\n\n51175.0\n\n\n0.721941\n\n\n0.192668\n\n\n1.0\n\n\n\n\n3\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\nUnstated\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n‚Ä¶\n\n\n1.0\n\n\n0.0\n\n\n0.888331\n\n\n0.010760\n\n\n0.279412\n\n\n2.65\n\n\n79269.0\n\n\n0.920431\n\n\n0.412142\n\n\n1.0\n\n\n\n\n4\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n$50,000\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n‚Ä¶\n\n\n0.0\n\n\n1.0\n\n\n0.759014\n\n\n0.127421\n\n\n0.442389\n\n\n1.85\n\n\n40908.0\n\n\n0.416072\n\n\n0.439965\n\n\n1.0\n\n\n\n\n\n5 rows √ó 51 columns\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate the mean donation rate for each group\ndonation_rates = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"Group\"] = donation_rates[\"treatment\"].map({1: \"Treatment\", 0: \"Control\"})\n\n# Create the barplot\nplt.figure(figsize=(6, 4))\nsns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n# Label the chart\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"Group\")\nplt.title(\"Donation Response Rate: Treatment vs Control\")\nplt.ylim(0, 0.03)  # Set y-axis range for visual clarity\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n/tmp/ipykernel_66588/1362242253.py:10: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n/tmp/ipykernel_1451/3587301648.py:10: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=donation_rates, x=\"Group\", y=\"gave\", palette=\"Blues_d\")\n\n\n\npng\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Load the dataset\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Prepare ratio indicators\ndf[\"ratio\"] = df[\"ratio\"].astype(\"str\")\ndf[\"ratio1\"] = (df[\"ratio\"] == \"1\").astype(int)\ndf[\"ratio2\"] = (df[\"ratio\"] == \"2\").astype(int)\ndf[\"ratio3\"] = (df[\"ratio\"] == \"3\").astype(int)\n\n# Filter for treatment group only (exclude control group)\ndf_treat_only = df[df[\"treatment\"] == 1].copy()\n\n# T-tests: 1:1 vs 2:1 and 2:1 vs 3:1\ngave_1to1 = df_treat_only[df_treat_only[\"ratio1\"] == 1][\"gave\"]\ngave_2to1 = df_treat_only[df_treat_only[\"ratio2\"] == 1][\"gave\"]\ngave_3to1 = df_treat_only[df_treat_only[\"ratio3\"] == 1][\"gave\"]\n\nt_1v2, p_1v2, _ = sm.stats.ttest_ind(gave_1to1, gave_2to1)\nt_2v3, p_2v3, _ = sm.stats.ttest_ind(gave_2to1, gave_3to1)\n\n# Regression using dummy variables\nreg_dummy = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=df_treat_only).fit()\n\n# Regression using categorical variable\ndf_treat_only[\"ratio\"] = df_treat_only[\"ratio\"].astype(\"category\")\nreg_cat = smf.ols(\"gave ~ C(ratio)\", data=df_treat_only).fit()\n\n# Group means\ngroup_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()\ndirect_diff_1v2 = group_means[\"2\"] - group_means[\"1\"]\ndirect_diff_2v3 = group_means[\"3\"] - group_means[\"2\"]\n\n# Regression differences\ncoef_1 = reg_dummy.params[\"ratio1\"]\ncoef_2 = reg_dummy.params[\"ratio2\"]\ncoef_3 = reg_dummy.params[\"ratio3\"]\nreg_diff_1v2 = coef_2 - coef_1\nreg_diff_2v3 = coef_3 - coef_2\n\n# Print summary\nprint(\"T-test p-value (1:1 vs 2:1):\", p_1v2)\nprint(\"T-test p-value (2:1 vs 3:1):\", p_2v3)\nprint(\"Direct difference in response rates (2:1 - 1:1):\", direct_diff_1v2)\nprint(\"Direct difference in response rates (3:1 - 2:1):\", direct_diff_2v3)\nprint(\"Regression-based difference (2:1 - 1:1):\", reg_diff_1v2)\nprint(\"Regression-based difference (3:1 - 2:1):\", reg_diff_2v3)\n\n# Optional: Show regression summaries\nprint(\"\\nOLS Regression with dummy variables:\")\nprint(reg_dummy.summary())\n\nprint(\"\\nOLS Regression with categorical variable:\")\nprint(reg_cat.summary())\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        03:03:32   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        03:03:32   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_66588/2038281865.py:33: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\nT-test p-value (1:1 vs 2:1): 0.3345316854972399\nT-test p-value (2:1 vs 3:1): 0.9600305283739325\nDirect difference in response rates (2:1 - 1:1): 0.0018842510217149944\nDirect difference in response rates (3:1 - 2:1): 0.00010002398025293902\nRegression-based difference (2:1 - 1:1): 0.0018842510217149805\nRegression-based difference (3:1 - 2:1): 0.00010002398025296677\n\nOLS Regression with dummy variables:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nOLS Regression with categorical variable:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   gave   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                    0.6454\nDate:                Thu, 24 Apr 2025   Prob (F-statistic):              0.524\nTime:                        02:00:40   Log-Likelihood:                 16688.\nNo. Observations:               33396   AIC:                        -3.337e+04\nDf Residuals:                   33393   BIC:                        -3.334e+04\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.0207      0.001     14.912      0.000       0.018       0.023\nC(ratio)[T.2]     0.0019      0.002      0.958      0.338      -0.002       0.006\nC(ratio)[T.3]     0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\nOmnibus:                    38963.957   Durbin-Watson:                   1.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):          2506478.937\nSkew:                           6.511   Prob(JB):                         0.00\nKurtosis:                      43.394   Cond. No.                         3.73\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/tmp/ipykernel_1451/3381687197.py:33: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  group_means = df_treat_only.groupby(\"ratio\")[\"gave\"].mean()"
  },
  {
    "objectID": "hw1_questions.html#does-match-size-affect-donation-rates",
    "href": "hw1_questions.html#does-match-size-affect-donation-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üß™ Does Match Size Affect Donation Rates?",
    "text": "üß™ Does Match Size Affect Donation Rates?\nWe test whether offering larger match ratios (e.g., 2:1 or 3:1) increases the likelihood of donating compared to a standard 1:1 match.\n\nüîç T-Test Results\n\n1:1 vs 2:1: No statistically significant difference (p ‚âà 0.335)\n2:1 vs 3:1: No statistically significant difference (p ‚âà 0.960)\n\n\n\nüìà Regression Results\nWe ran two regressions: 1. Using separate dummy variables (ratio1, ratio2, ratio3) ‚Äî one for each match level 2. Using a single categorical variable (C(ratio))\nBoth approaches yielded similar results: - Donation rate for 1:1 ‚âà 2.07% - Donation rate for 2:1 ‚âà 2.26% - Donation rate for 3:1 ‚âà 2.27% - Differences between them are very small and not statistically significant\n\n\nüìä Direct Comparison of Response Rates\n\n2:1 ‚Äì 1:1 ‚âà +0.19 percentage points\n3:1 ‚Äì 2:1 ‚âà +0.01 percentage points These findings match the regression results.\n\n\n\n‚úÖ Conclusion\nThese results support the authors‚Äô observatin that ‚ÄúThe figures suggest that larger match ratios have no additional impact.‚Äù\nüí° Key Insight: Donors respond to the presence of a match, but increasing the size of the match does not further increase the likelihood of donating.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter only those who made a donation\ndf_donors = df[df[\"gave\"] == 1]\n\n# Set up the plotting area\nplt.figure(figsize=(12, 5))\n\n# Histogram for Control Group\nplt.subplot(1, 2, 1)\ncontrol_amounts = df_donors[df_donors[\"treatment\"] == 0][\"amount\"]\nsns.histplot(control_amounts, bins=30, kde=False, color=\"skyblue\")\nplt.axvline(control_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${control_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Control Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Histogram for Treatment Group\nplt.subplot(1, 2, 2)\ntreatment_amounts = df_donors[df_donors[\"treatment\"] == 1][\"amount\"]\nsns.histplot(treatment_amounts, bins=30, kde=False, color=\"lightgreen\")\nplt.axvline(treatment_amounts.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${treatment_amounts.mean():.2f}')\nplt.title(\"Donation Amounts - Treatment Group\")\nplt.xlabel(\"Amount Donated\")\nplt.ylabel(\"Number of Donors\")\nplt.legend()\n\n# Final layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npng"
  },
  {
    "objectID": "hw1_questions.html#size-of-charitable-contribution",
    "href": "hw1_questions.html#size-of-charitable-contribution",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üíµ Size of Charitable Contribution",
    "text": "üíµ Size of Charitable Contribution\n\n‚úÖ Q1: Does treatment affect donation amount (all individuals)?\nWe performed both a t-test and a bivariate linear regression on the full dataset.\n\nT-test p-value: ~0.063\nRegression coefficient: +0.15 (Treatment vs.¬†Control)\nüìâ Interpretation:\n\nThe treatment group gave slightly more on average, but the difference is not statistically significant at the 5% level.\nThis suggests that while the match offer encourages more people to donate, it does not meaningfully affect how much they give, on average, across the full sample.\n\n\n\n\n\n‚úÖ Q2: Does treatment affect donation amount among donors only?\nWe repeated the analysis only for individuals who made a donation (i.e., gave == 1).\n\nT-test p-value: ~0.561\nRegression coefficient: ‚Äì1.67\nüìâ Interpretation:\n\nAmong donors, there is no statistically significant difference in how much was donated between the treatment and control groups.\nInterestingly, the control group gave slightly more, but this difference is small and not reliable.\n‚ö†Ô∏è Causal Note: This analysis does not have a causal interpretation, because it conditions on making a donation ‚Äî a behavior affected by the treatment. This introduces selection bias.\n\n\n\n\n\n‚úÖ Q3: What do the histograms show?\nWe created histograms of donation amounts among donors, separately for the treatment and control groups. Each plot includes:\n\nA red dashed line indicating the mean donation.\nThe distributions are highly right-skewed, with many small gifts and a few large ones.\n\nüìä Observations: - The average donation amount is very similar across the groups. - Most donations are clustered around $10‚Äì$50. - There is no visual evidence that treatment led to larger donations.\n\n\n\nüß† Final Takeaway:\nOffering a matching grant increases the probability of giving, but among those who give, it does not increase the amount given. This suggests that match offers primarily work as a participation nudge, not a generosity multiplier.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 10,000 Bernoulli trials\ncontrol_draws = np.random.binomial(1, 0.018, size=10000)\ntreatment_draws = np.random.binomial(1, 0.022, size=10000)\n\n# Calculate the difference at each draw\ndifferences = treatment_draws - control_draws\n\n# Compute the cumulative average of the differences\ncumulative_average = np.cumsum(differences) / np.arange(1, 10001)\n\n# Plot the cumulative average\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_average, label=\"Cumulative Average of Differences\")\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True Treatment Effect (0.004)\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average\")\nplt.title(\"Law of Large Numbers: Cumulative Average of Treatment - Control\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\npng"
  },
  {
    "objectID": "hw1_questions.html#law-of-large-numbers-simulating-donation-response-rates",
    "href": "hw1_questions.html#law-of-large-numbers-simulating-donation-response-rates",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üìà Law of Large Numbers: Simulating Donation Response Rates",
    "text": "üìà Law of Large Numbers: Simulating Donation Response Rates\nIn this simulation, we illustrate the Law of Large Numbers using synthetic data inspired by the charitable giving experiment.\n\nüéØ Setup:\n\nControl Group: Simulated with a Bernoulli distribution where the probability of donating is 1.8% (p = 0.018)\nTreatment Group: Simulated with a Bernoulli distribution where the probability of donating is 2.2% (p = 0.022)\nWe simulate 10,000 draws from each group and compute the difference (Treatment ‚Äì Control) for each pair.\nThen, we plot the cumulative average of those differences over time.\n\n\n\nüìä What the Graph Shows:\n\nThe line begins noisy due to early randomness.\nAs the number of draws increases, the average stabilizes and converges around the true treatment effect of 0.004 (2.2% - 1.8%).\nThe red dashed line marks this theoretical value.\n\n\n\nüß† Interpretation:\nThis visualization demonstrates the Law of Large Numbers: &gt; As the sample size grows, the sample average of a statistic will converge to its true population value.\nThis underlines why large-scale experiments (like the one in the Karlan & List paper) are powerful: with enough data, we can estimate effects reliably despite inherent randomness in individual behavior.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Function to simulate the distribution of mean differences\ndef simulate_diff_distribution(sample_size, reps=1000):\n    differences = []\n    for _ in range(reps):\n        control_sample = np.random.binomial(1, 0.018, size=sample_size)\n        treatment_sample = np.random.binomial(1, 0.022, size=sample_size)\n        diff = treatment_sample.mean() - control_sample.mean()\n        differences.append(diff)\n    return differences\n\n# Sample sizes\nsample_sizes = [50, 200, 500, 1000]\n\n# Generate and plot each histogram separately\nfor size in sample_sizes:\n    diffs = simulate_diff_distribution(sample_size=size)\n    plt.figure(figsize=(7, 4))\n    sns.histplot(diffs, bins=30, kde=False, color=\"skyblue\")\n    plt.axvline(0.004, color='red', linestyle='--', linewidth=2, label=\"True Effect = 0.004\")\n    plt.title(f\"Sampling Distribution of Mean Differences (n = {size})\")\n    plt.xlabel(\"Mean Difference (Treatment - Control)\")\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng"
  },
  {
    "objectID": "hw1_questions.html#central-limit-theorem-demonstration",
    "href": "hw1_questions.html#central-limit-theorem-demonstration",
    "title": "A Replication of Karlan and List (2007)",
    "section": "üìä Central Limit Theorem Demonstration",
    "text": "üìä Central Limit Theorem Demonstration\nThis section visually demonstrates the Central Limit Theorem (CLT) using simulations based on the charitable giving experiment setup.\n\nüî¨ Method:\nFor each sample size ‚Äî n = 50, 200, 500, 1000: 1. We simulate 1,000 experiments. 2. In each experiment: - Take n samples from the control group (Bernoulli, p = 0.018) - Take n samples from the treatment group (Bernoulli, p = 0.022) - Compute the average difference in donation rates: treatment_mean - control_mean 3. We plot the histogram of the 1,000 average differences.\n\n\nüìà Interpretation of Histograms:\n\nFor n = 50, the distribution is wide and irregular ‚Äî highly affected by sampling noise.\nAs n increases (200, 500, 1000):\n\nThe distribution becomes tighter and smoother\nIt becomes centered around the true effect of 0.004 (shown by a red dashed line).\nThe shape begins to resemble a normal distribution.\n\n\n\n\nüß† Why It Matters:\nThis simulation illustrates the Central Limit Theorem: &gt; As sample size increases, the sampling distribution of the sample mean becomes approximately normal ‚Äî even when the underlying data are not normally distributed.\n‚úÖ Takeaway:\nThanks to the CLT, we can use normal-based inference methods (like t-tests and regression) when we have large enough samples ‚Äî as in the Karlan & List field experiment."
  }
]